{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Web scraping the sites that contain site maps\n",
    "\n",
    "This notebook aims to gather all the data from multiple websites that contain site maps. The target is to get as many product websites from different pages with as many different furniture types as possible."
   ],
   "id": "7c6a352b9f6cb4a9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# importing libraries\n",
    "\n",
    "import csv \n",
    "import operator\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Methods for getting the base url and for checking if the website has a sitemap",
   "id": "82c89620a1731cc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "SITEMAP_PATHS = [\n",
    "    \"sitemap.xml\",\n",
    "    \"sitemap_index.xml\",\n",
    "    \".sitemap.xml\",\n",
    "    \"sitemap/sitemap.xml\",\n",
    "    \"sitemap_index/sitemap.xml\"\n",
    "]\n",
    "\n",
    "def get_base_url(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        return base_url\n",
    "    except Exception as e:\n",
    "        # print(f\"Error parsing URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_sitemap(base_url):\n",
    "    \"\"\"\n",
    "    Check if the given base URL has a sitemap in one of the common paths.\n",
    "    Returns the URL of the sitemap if found, otherwise None.\n",
    "    \"\"\"\n",
    "    for sitemap_path in SITEMAP_PATHS:\n",
    "        sitemap_url = urljoin(base_url, sitemap_path)\n",
    "        try:\n",
    "            response = requests.head(sitemap_url, timeout=10)\n",
    "            # Check if the URL exists and returns a successful status code (200)\n",
    "            if 200 <= response.status_code < 300:\n",
    "                return sitemap_url\n",
    "        except requests.RequestException as e:\n",
    "            # print(f\"Error checking {sitemap_url}: {e}\")\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def check_sitemap_concurrently(base_urls):\n",
    "    \"\"\"\n",
    "    Checks sitemaps for a list of base URLs concurrently.\n",
    "    Returns a list of results with the base URL and sitemap URL.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        futures = {executor.submit(check_sitemap, base_url): base_url for base_url in base_urls}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Checking sitemaps\"):\n",
    "            base_url = futures[future]\n",
    "            sitemap_url = future.result()\n",
    "            if sitemap_url:\n",
    "                results.append([base_url, \"Sitemap found\", sitemap_url])\n",
    "            else:\n",
    "                results.append([base_url, \"No sitemap found\", \"\"])\n",
    "    return results"
   ],
   "id": "be2f786884fe403e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filtering out the websites that are not accessible that have a site map",
   "id": "f654312a4e74e8f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# reading the csv file and storing the links in the links list\n",
    "links = []\n",
    "with open('Data/furniture stores pages.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        try:\n",
    "            link = get_base_url(row[0])\n",
    "            links.append(link)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "\n",
    "# Check sitemaps concurrently\n",
    "sitemap_results = check_sitemap_concurrently(links)\n",
    "\n",
    "# Write the results to the output CSV file\n",
    "output_csv = \"sitemap_results.csv\"\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([\"URL\", \"Status\", \"Sitemap URL\"])  # Header row\n",
    "    csvwriter.writerows(sitemap_results)  # Write all results\n",
    "\n",
    "print(f\"Results saved to {output_csv}.\")"
   ],
   "id": "cbdd1c3171951b50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Getting the sitemaps from the csv file",
   "id": "53637297a4e4279d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# getting only the sitemaps from the csv file\n",
    "sitemaps = []\n",
    "with open('sitemap_results.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if row[1] != 'No sitemap found':\n",
    "            sitemaps.append(row[2])\n",
    "\n",
    "print(sitemaps[1:], len(sitemaps))"
   ],
   "id": "ddab114938e8c622",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Methods for getting the links from the sitemaps",
   "id": "7212ef201c988cf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_data(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "    return response.text\n",
    "\n",
    "def is_valid_product_link(url):\n",
    "    # Exclude common unwanted patterns\n",
    "    unwanted_patterns = ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.css', '.js', '.ico', 'tel:', 'mailto:', '#']\n",
    "    for pattern in unwanted_patterns:\n",
    "        if pattern in url:\n",
    "            return False\n",
    "    # Only accept URLs that contain \"collections\" or \"products\"\n",
    "    if '/products/' in url: # or  'collections' in url: - right now I will only focus on the products path\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_desired_site_map_link(url): #  \n",
    "    # Only accept URLs that contain \"sitemap\"\n",
    "    if 'sitemap_products_1.xml' in url: # most website have this in the url that I am looking for\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "dict_href_links = {}\n",
    "\n",
    "def get_links_from_sitemap(website_link): # modified version from the one in the other notebook\n",
    "    # Set the base of the URL depending on whether \"collections\" or \"products\" is in the link\n",
    "    website_origin = website_link\n",
    "    sitemap_index = website_link.find('/sitemap')\n",
    "    if sitemap_index == -1:\n",
    "        sitemap_index = website_link.find('/.sitemap')\n",
    "    if sitemap_index != -1:\n",
    "        website_origin = website_link[:sitemap_index + 1] \n",
    "\n",
    "    html_data = get_data(website_link)\n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    list_links = []\n",
    "\n",
    "    for link in soup.find_all(\"loc\"): # this contains the links inside xml files\n",
    "        link = link.text\n",
    "        # Filter out invalid links (non-product/collection pages)\n",
    "        \n",
    "        if not is_valid_product_link(link) and not is_desired_site_map_link(link): \n",
    "            continue\n",
    "        \n",
    "        link_to_append = None\n",
    "\n",
    "        # Handle absolute URLs that start with the origin\n",
    "        if link.startswith(str(website_origin)):\n",
    "            link_to_append = link\n",
    "        \n",
    "        # Handle relative URLs that start with \"/\"\n",
    "        elif link.startswith(\"/\"):\n",
    "            #print(href)\n",
    "            link_with_www = website_origin + link[1:]\n",
    "            #print(\"adjusted link =\", link_with_www)\n",
    "            link_to_append = link_with_www\n",
    "        \n",
    "\n",
    "        \n",
    "        # If link_to_append is not None, check if it's already in dict_href_links and if it's accessible\n",
    "        if link_to_append is not None:\n",
    "            if link_to_append not in dict_href_links: #  and check_website(link_to_append) - I will not check the links here, I will check them after I get all the links\n",
    "                dict_href_links[link_to_append] = None  # Mark it as seen\n",
    "\n",
    "                list_links.append(link_to_append)\n",
    "\n",
    "    # Convert list of links to a dictionary with \"Not-checked\" as the default value for each\n",
    "    dict_links = dict.fromkeys(list_links, \"Not-checked\")\n",
    "    return dict_links\n",
    "\n",
    "def get_subpage_links(l, max_depth=3, current_depth=0, write_frequency=500, csv_filename=\"link_data.csv\"):\n",
    "    processed_links_count = 0\n",
    "    \n",
    "    if current_depth >= max_depth:\n",
    "        return l\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        futures = {executor.submit(get_links_from_sitemap, link): link for link in l if l[link] == \"Not-checked\"}\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing subpage links\"):\n",
    "            link = futures[future]\n",
    "            try:\n",
    "                dict_links_subpages = future.result()\n",
    "                # print(f\"Processed {link} with {len(dict_links_subpages)} subpages.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                continue\n",
    "            l[link] = \"Checked\"\n",
    "            l.update(dict_links_subpages)\n",
    "\n",
    "            processed_links_count += 1\n",
    "            \n",
    "            # Write to file every 'write_frequency' processed links\n",
    "            if processed_links_count >= write_frequency: # this actually writes all the links to the csv file - even the not checked ones but in my case it is sufficient\n",
    "                write_links_to_csv(l, csv_filename)\n",
    "                processed_links_count = 0  # Reset the counter\n",
    "\n",
    "    # Recursively call the function for the next depth level\n",
    "    return get_subpage_links(l, max_depth, current_depth + 1, write_frequency, csv_filename)\n",
    "\n",
    "def write_links_to_csv(links_dict, csv_filename):\n",
    "    \"\"\"Writes the current state of the links dictionary to a CSV file.\"\"\"\n",
    "    with open(csv_filename, \"w\", newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        for link in links_dict.keys():\n",
    "            csvwriter.writerow([link])\n",
    "\n",
    "    print(f\"Links saved to {csv_filename}.\")\n"
   ],
   "id": "340ba3461c63f04a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Getting the links from the sitemaps\n",
   "id": "edc385befce0d660"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we gather all the links from some pages\n",
    "\n",
    "# we test with the first page inside the final_links csv\n",
    "\n",
    "websites = sitemaps[1:]  # exclude the first row\n",
    "# create dictionary of website\n",
    "dict_links = { website : \"Not-checked\" for website in websites }\n",
    "\n",
    "counter, counter2 = None, 0\n",
    "csv_filename = \"link_data.csv\"\n",
    "\n",
    "while counter != 0:\n",
    "    counter2 += 1\n",
    "    dict_links2 = get_subpage_links(dict_links, csv_filename=csv_filename)\n",
    "    counter = operator.countOf(dict_links2.values(), \"Not-checked\")  # Number of \"Not-checked\" links\n",
    "    \n",
    "    # Print some statements for debugging\n",
    "    print(\"\")\n",
    "    print(\"THIS IS LOOP ITERATION NUMBER\", counter2)\n",
    "    print(\"LENGTH OF DICTIONARY WITH LINKS =\", len(dict_links2))\n",
    "    print(\"NUMBER OF 'Not-checked' LINKS = \", counter)\n",
    "    print(\"\")\n",
    "    \n",
    "    dict_links = dict_links2\n",
    "    \n",
    "\n",
    "write_links_to_csv(dict_links, csv_filename)\n",
    "\n",
    "print(\"Links saved to link_data.csv.\")\n",
    "\n",
    "# removing any link that points to a sitemap\n",
    "\n"
   ],
   "id": "1d9434319c325b31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Removing the sitemap links from the csv file",
   "id": "39f6c9b2985b3b46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "links = []\n",
    "\n",
    "with open('link_data.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if '.xml' not in row[0]:\n",
    "            links.append(row[0])\n",
    "\n",
    "with open('link_data.csv', mode='w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    for link in links:\n",
    "        csv_writer.writerow([link])\n",
    "        \n",
    "print(\"Links saved to link_data.csv.\")"
   ],
   "id": "59753b24e3dcc7eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# DECIDED TO NOT USE THIS METHOD \n",
    "### Creating a new csv file with the links content \n",
    "\n",
    "First csv that will contain only raw html data (from h1, p tags etc.)\n",
    "- URL,Source,title,h1,h2,h3,p,span\n",
    "\n",
    "Second csv will contain the links will contain a more finallized version of the data\n",
    "- URL,Source,Product_Name,Description,Price"
   ],
   "id": "57fa53806b8a73e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# tags_to_extract = ['h1', 'p'] # change this in order to get information from different tags\n",
    "# \n",
    "# output_file = 'raw_content.csv'\n",
    "# \n",
    "# def extract_text_from_url(url):\n",
    "#     html_data = get_data(url) # this is \"None\" if the link is not accessible\n",
    "#     if html_data is None:\n",
    "#         return None\n",
    "#     soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "#     \n",
    "#     extracted_text = { tag: [] for tag in tags_to_extract }\n",
    "#     \n",
    "#     for tag in tags_to_extract:\n",
    "#         elements = soup.find_all(tag)\n",
    "#         for element in elements:\n",
    "#             text = element.get_text(strip=True)\n",
    "#             if text:\n",
    "#                 extracted_text[tag].append(text)\n",
    "#     for tag in extracted_text:\n",
    "#         extracted_text[tag] = ' ### '.join(extracted_text[tag])\n",
    "#         \n",
    "#     return extracted_text\n",
    "# \n",
    "# def process_url(url):\n",
    "#     extracted_text = extract_text_from_url(url)\n",
    "#     if extracted_text:\n",
    "#         row = [url, get_base_url(url)]  # Replace 'Source Name' with your actual source\n",
    "#         for tag in tags_to_extract:\n",
    "#             row.append(extracted_text.get(tag, ''))  # Append text for each tag\n",
    "#         return row\n",
    "#     return None\n",
    "# \n",
    "# def save_text_to_csv(data):\n",
    "#     with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         headers = ['URL', 'Source'] + tags_to_extract\n",
    "#         writer.writerow(headers)\n",
    "#         for row in data:\n",
    "#             if row[2] and row[3]:\n",
    "#                 writer.writerow(row)"
   ],
   "id": "560a06202edd9f12",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Extracting all relevant text from the links and maintaining its structure\n",
    "\n",
    "In this code block I will take a different approach and get all relevant data from the page and not altering its ordering (like I did above by separating the tags content apart). If this proves inefficient I will go back to the previous method but this makes more sense logically as long as the text segments are short enough for the model to understand, yet not to small for the model to not be able to understand the context (even though the context is some random text from the page like hyperlink text etc.)"
   ],
   "id": "cb886e2cb6e99b70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T21:10:09.457966Z",
     "start_time": "2024-09-04T21:10:09.451901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_text_from_url(url):\n",
    "    html_data = get_data(url)\n",
    "    if html_data is None:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "\n",
    "    # Remove scripts, styles, and irrelevant content\n",
    "    for script in soup([\"script\", \"style\", \"footer\", \"nav\", \"header\", \"noscript\"]):\n",
    "        script.extract()\n",
    "\n",
    "    # Extract text while keeping the structure\n",
    "    text_blocks = set()  # Use a set to store unique text blocks\n",
    "    for element in soup.find_all(['h1', 'h2', 'h3', 'p', 'a', 'li', 'span', 'div']):\n",
    "        text = element.get_text(separator=\" \", strip=True)\n",
    "        if text:\n",
    "            text_blocks.add(text)\n",
    "\n",
    "    # Join the text blocks to maintain structure, using ' ### ' as separator for clarity\n",
    "    return ' ### '.join(text_blocks)\n",
    "\n",
    "# Process a single URL and return a row with extracted text\n",
    "def process_url(url):\n",
    "    extracted_text = extract_text_from_url(url)\n",
    "    if extracted_text:\n",
    "        return [url, get_base_url(url), extracted_text]\n",
    "    return None\n",
    "\n",
    "# Save extracted text data into a CSV file\n",
    "def save_text_to_csv(data, output_file='raw_content.csv'):\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['URL', 'Source', 'Content'])\n",
    "        for row in data:\n",
    "            if row and row[2]:  # Ensure that content is not empty\n",
    "                writer.writerow(row)\n"
   ],
   "id": "7afeda848fbf46de",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T21:10:11.632098Z",
     "start_time": "2024-09-04T21:10:11.021615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GETTING THE LINKS FROM 'link_data.csv'\n",
    "\n",
    "urls = []\n",
    "\n",
    "with open('link_data.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        urls.append(row[0])\n",
    "        \n",
    "import random\n",
    "random.shuffle(urls) \n"
   ],
   "id": "a3eb5b03bb419dee",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T21:11:35.491961Z",
     "start_time": "2024-09-04T21:10:19.497785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "extracted_data = []\n",
    "    \n",
    "\n",
    "with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "    future_to_url = {executor.submit(process_url, url): url for url in urls[:1000]}\n",
    "    for future in tqdm(concurrent.futures.as_completed(future_to_url), total=len(future_to_url), desc=\"Extracting content from URLs (p, h1, h2 etc.)\"): \n",
    "        result = future.result()\n",
    "        if result:\n",
    "            extracted_data.append(result)\n"
   ],
   "id": "3ea0363b4856970a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting content from URLs (p, h1, h2 etc.): 100%|██████████| 1000/1000 [01:15<00:00, 13.17it/s]\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T21:12:12.705638Z",
     "start_time": "2024-09-04T21:12:12.041785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the extracted content to a CSV file\n",
    "save_text_to_csv(extracted_data)\n",
    "print(f\"Extraction completed. Data saved to {'raw_content.csv'}.\")"
   ],
   "id": "14e35c0ca21c7454",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed. Data saved to raw_content.csv.\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d0ecc46f4aa812a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "65412ca13beb6924"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
