{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Web scraping for most of the sitemap websites"
   ],
   "id": "e58d6fa6cae01bda"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# importing libraries\n",
    "\n",
    "import csv \n",
    "import operator\n",
    "import re\n",
    "import threading\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import spacy # we use this for word similarity\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading the links for scraping"
   ],
   "id": "70014b6139f4992b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "links = []\n",
    "\n",
    "with open('../dispersed_link_data.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        links.append(row[0])"
   ],
   "id": "b45c1a630cbb50ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Additional methods for storing data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce5baaf9812c5c3a"
  },
  {
   "cell_type": "code",
   "source": [
    "def write_data_to_csv(data_to_write, csv_filename):\n",
    "    # Open the file in append mode 'a' and ensure newline is handled correctly\n",
    "    with open(csv_filename, \"a\", newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        # Iterate over the list of tuples and append each to the CSV\n",
    "        for row in data_to_write:\n",
    "            csvwriter.writerow(row)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e2c507de6cfaec1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Methods that will be used for scraping\n"
   ],
   "id": "7e730773ae77fcc8"
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "def get_base_url(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        return base_url\n",
    "    except Exception as e:\n",
    "        # print(f\"Error parsing URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace multiple spaces and line breaks with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:79.0) Gecko/20100101 Firefox/79.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "]\n",
    "\n",
    "# List of proxy servers (this can be dynamically updated with proxy pools or services)\n",
    "PROXIES = [\n",
    "    {\"http\": \"http://178.48.68.61:18080\",  \"https\": \"http://178.48.68.61:18080\"},    \n",
    "]\n",
    "\n",
    "# Function to get page content\n",
    "def get_data(url):\n",
    "    headers = {\"User-Agent\": random.choice(USER_AGENTS)}  # Rotate user-agent\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=3)\n",
    "\n",
    "        # Handle rate-limiting (HTTP 429) by pausing and retrying\n",
    "        if response.status_code == 429:\n",
    "            tqdm.write(f\"FROM GET_DATA: Rate limit reached. Sleeping before retrying {url}\")\n",
    "            # print(f\"FROM GET_DATA: Rate limit reached. Sleeping before retrying {url}\")\n",
    "            time.sleep(random.uniform(4, 8))  # Random delay to avoid detection\n",
    "            return get_data(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.content  # Return HTML content if successful\n",
    "\n",
    "        tqdm.write(f\"FROM GET_DATA: Failed to retrieve {url}, Status Code: {response.status_code}\")\n",
    "        # print(f\"FROM GET_DATA: Failed to retrieve {url}, Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        tqdm.write(f\"FROM GET_DATA: Error fetching {url}: {e}\")\n",
    "        # print(f\"FROM GET_DATA: Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def has_letters(input_string):\n",
    "    return any(char.isalpha() for char in input_string)\n",
    "\n",
    "def extract_data(url):\n",
    "    html_data = get_data(url)\n",
    "    if html_data is None:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    \n",
    "    # Finding the title in the meta tags\n",
    "    title = soup.find('title')\n",
    "    if title:\n",
    "        title = title.get_text()\n",
    "    else: title = None\n",
    "        \n",
    "    # Remove scripts, styles, and irrelevant content\n",
    "    for script in soup([\"script\", \"style\", \"footer\", \"nav\", \"header\", \"noscript\", \"head\"]):\n",
    "        script.extract()\n",
    "\n",
    "    h1_tags = [clean_text(h1.get_text()) for h1 in soup.find_all('h1')]\n",
    "    \n",
    "    page_text = clean_text(soup.get_text(separator=' '))\n",
    "    \n",
    "    h1_tag = None\n",
    "    # If no h1 tags are found, return None\n",
    "    if h1_tags:\n",
    "        h1_tag = h1_tags[0] \n",
    " \n",
    "    # We get the last path of the url\n",
    "    url_index = url.rfind('/')\n",
    "    url_last_path = url[url_index + 1:].replace('-', ' ').replace('_', ' ')\n",
    "    \n",
    "    if not has_letters(url_last_path):\n",
    "        url_last_path = None\n",
    "    \n",
    "    h1_tag_position = None\n",
    "    if h1_tag and page_text:  # Ensure both are not None\n",
    "        if h1_tag in page_text:\n",
    "            start_idx = page_text.index(h1_tag)\n",
    "            end_idx = start_idx + len(h1_tag)\n",
    "            h1_tag_position = (h1_tag, start_idx, end_idx)\n",
    "    \n",
    "    return h1_tag_position, title, url_last_path, page_text\n",
    "    \n",
    "\n",
    "final_data = []\n",
    "links = links[:100000]  \n",
    "\n",
    "final_data_lock = threading.Lock()  # A lock to ensure thread-safe appending to the list\n",
    "\n",
    "# Function to process a single URL\n",
    "def process_url(link):\n",
    "    try:\n",
    "        result = extract_data(link)\n",
    "        if result is not None:\n",
    "            h1_tag, title, url_last_path, page_text = result\n",
    "            with final_data_lock:\n",
    "                final_data.append([link, h1_tag, title, url_last_path, page_text])\n",
    "        else:\n",
    "            tqdm.write(f\"FROM:PROCESS_URL: Skipped processing {link} because extract_data returned None\")\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"FROM:PROCESS_URL: Error processing {link}: {e}\")\n",
    "\n",
    "\n",
    "# Read from CSV and process in parallel\n",
    "with open('../dispersed_link_data.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:  # Adjust max_workers as needed\n",
    "        futures = []\n",
    "        for link in links:\n",
    "            futures.append(executor.submit(process_url, link))  # Submit tasks to the thread pool\n",
    "\n",
    "        # Track progress with tqdm\n",
    "        for _ in tqdm(concurrent.futures.as_completed(futures), total=len(links), desc=\"Processing URLs\"):\n",
    "            pass  # We don't need the result here since we're appending directly to final_data\n",
    "\n",
    "                          \n"
   ],
   "id": "da2a0de946c3761f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with open('../data/preprocessed_data_from_all_sitemaps.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    csv_writer.writerow(['URL', 'h1', 'title', 'url_last_path', 'page_text'])\n",
    "    for row in final_data:\n",
    "        csv_writer.writerow(row)\n",
    "        \n",
    "\n",
    "print(\"Processing completed!\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6c10df05c3a6f7a2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
