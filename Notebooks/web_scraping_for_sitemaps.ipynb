{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Web scraping the sites that contain site maps\n",
    "\n",
    "This notebook aims to gather all the data from multiple websites that contain site maps. The target is to get as many product websites from different pages with as many different furniture types as possible."
   ],
   "id": "7c6a352b9f6cb4a9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# importing libraries\n",
    "\n",
    "import csv \n",
    "import operator\n",
    "import re\n",
    "import threading\n",
    "\n",
    "import sys\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import spacy # we use this for word similarity\n",
    "\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Methods for getting the base url and for checking if the website has a sitemap",
   "id": "82c89620a1731cc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "SITEMAP_PATHS = [\n",
    "    \"sitemap.xml\",\n",
    "    \"sitemap_index.xml\",\n",
    "    \".sitemap.xml\",\n",
    "    \"sitemap/sitemap.xml\",\n",
    "    \"sitemap_index/sitemap.xml\"\n",
    "]\n",
    "\n",
    "def get_base_url(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        return base_url\n",
    "    except Exception as e:\n",
    "        # print(f\"Error parsing URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_sitemap(base_url):\n",
    "    \"\"\"\n",
    "    Check if the given base URL has a sitemap in one of the common paths.\n",
    "    Returns the URL of the sitemap if found, otherwise None.\n",
    "    \"\"\"\n",
    "    for sitemap_path in SITEMAP_PATHS:\n",
    "        sitemap_url = urljoin(base_url, sitemap_path)\n",
    "        try:\n",
    "            response = requests.head(sitemap_url, timeout=10)\n",
    "            # Check if the URL exists and returns a successful status code (200)\n",
    "            if 200 <= response.status_code < 300:\n",
    "                return sitemap_url\n",
    "        except requests.RequestException as e:\n",
    "            # print(f\"Error checking {sitemap_url}: {e}\")\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def check_sitemap_concurrently(base_urls):\n",
    "    \"\"\"\n",
    "    Checks sitemaps for a list of base URLs concurrently.\n",
    "    Returns a list of results with the base URL and sitemap URL.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        futures = {executor.submit(check_sitemap, base_url): base_url for base_url in base_urls}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Checking sitemaps\"):\n",
    "            base_url = futures[future]\n",
    "            sitemap_url = future.result()\n",
    "            if sitemap_url:\n",
    "                results.append([base_url, \"Sitemap found\", sitemap_url])\n",
    "            else:\n",
    "                results.append([base_url, \"No sitemap found\", \"\"])\n",
    "    return results"
   ],
   "id": "be2f786884fe403e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filtering out the websites that are not accessible that have a site map",
   "id": "f654312a4e74e8f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# reading the csv file and storing the links in the links list\n",
    "links = []\n",
    "with open('Data/furniture stores pages.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        try:\n",
    "            link = get_base_url(row[0])\n",
    "            links.append(link)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "\n",
    "# Check sitemaps concurrently\n",
    "sitemap_results = check_sitemap_concurrently(links)\n",
    "\n",
    "# Write the results to the output CSV file\n",
    "output_csv = \"sitemap_results.csv\"\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([\"URL\", \"Status\", \"Sitemap URL\"])  # Header row\n",
    "    csvwriter.writerows(sitemap_results)  # Write all results\n",
    "\n",
    "print(f\"Results saved to {output_csv}.\")"
   ],
   "id": "cbdd1c3171951b50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Getting the sitemaps from the csv file",
   "id": "53637297a4e4279d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# getting only the sitemaps from the csv file\n",
    "sitemaps = []\n",
    "with open('sitemap_results.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if row[1] != 'No sitemap found':\n",
    "            sitemaps.append(row[2])\n",
    "\n",
    "print(sitemaps[1:], len(sitemaps))"
   ],
   "id": "ddab114938e8c622",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Methods for getting the links from the sitemaps",
   "id": "7212ef201c988cf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_data(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "    return response.text\n",
    "\n",
    "def is_valid_product_link(url):\n",
    "    # Exclude common unwanted patterns\n",
    "    unwanted_patterns = ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.css', '.js', '.ico', 'tel:', 'mailto:', '#']\n",
    "    for pattern in unwanted_patterns:\n",
    "        if pattern in url:\n",
    "            return False\n",
    "    # Only accept URLs that contain \"collections\" or \"products\"\n",
    "    if '/products/' in url: # or  'collections' in url: - right now I will only focus on the products path\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_desired_site_map_link(url): #  \n",
    "    # Only accept URLs that contain \"sitemap\"\n",
    "    if 'sitemap_products_1.xml' in url: # most website have this in the url that I am looking for\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "dict_href_links = {}\n",
    "\n",
    "def get_links_from_sitemap(website_link): # modified version from the one in the other notebook\n",
    "    # Set the base of the URL depending on whether \"collections\" or \"products\" is in the link\n",
    "    website_origin = website_link\n",
    "    sitemap_index = website_link.find('/sitemap')\n",
    "    if sitemap_index == -1:\n",
    "        sitemap_index = website_link.find('/.sitemap')\n",
    "    if sitemap_index != -1:\n",
    "        website_origin = website_link[:sitemap_index + 1] \n",
    "\n",
    "    html_data = get_data(website_link)\n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    list_links = []\n",
    "\n",
    "    for link in soup.find_all(\"loc\"): # this contains the links inside xml files\n",
    "        link = link.text\n",
    "        # Filter out invalid links (non-product/collection pages)\n",
    "        \n",
    "        if not is_valid_product_link(link) and not is_desired_site_map_link(link): \n",
    "            continue\n",
    "        \n",
    "        link_to_append = None\n",
    "\n",
    "        # Handle absolute URLs that start with the origin\n",
    "        if link.startswith(str(website_origin)):\n",
    "            link_to_append = link\n",
    "        \n",
    "        # Handle relative URLs that start with \"/\"\n",
    "        elif link.startswith(\"/\"):\n",
    "            #print(href)\n",
    "            link_with_www = website_origin + link[1:]\n",
    "            #print(\"adjusted link =\", link_with_www)\n",
    "            link_to_append = link_with_www\n",
    "        \n",
    "\n",
    "        \n",
    "        # If link_to_append is not None, check if it's already in dict_href_links and if it's accessible\n",
    "        if link_to_append is not None:\n",
    "            if link_to_append not in dict_href_links: #  and check_website(link_to_append) - I will not check the links here, I will check them after I get all the links\n",
    "                dict_href_links[link_to_append] = None  # Mark it as seen\n",
    "\n",
    "                list_links.append(link_to_append)\n",
    "\n",
    "    # Convert list of links to a dictionary with \"Not-checked\" as the default value for each\n",
    "    dict_links = dict.fromkeys(list_links, \"Not-checked\")\n",
    "    return dict_links\n",
    "\n",
    "def get_subpage_links(l, max_depth=3, current_depth=0, write_frequency=500, csv_filename=\"link_data.csv\"):\n",
    "    processed_links_count = 0\n",
    "    \n",
    "    if current_depth >= max_depth:\n",
    "        return l\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        futures = {executor.submit(get_links_from_sitemap, link): link for link in l if l[link] == \"Not-checked\"}\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing subpage links\"):\n",
    "            link = futures[future]\n",
    "            try:\n",
    "                dict_links_subpages = future.result()\n",
    "                # print(f\"Processed {link} with {len(dict_links_subpages)} subpages.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                continue\n",
    "            l[link] = \"Checked\"\n",
    "            l.update(dict_links_subpages)\n",
    "\n",
    "            processed_links_count += 1\n",
    "            \n",
    "            # Write to file every 'write_frequency' processed links\n",
    "            if processed_links_count >= write_frequency: # this actually writes all the links to the csv file - even the not checked ones but in my case it is sufficient\n",
    "                write_links_to_csv(l, csv_filename)\n",
    "                processed_links_count = 0  # Reset the counter\n",
    "\n",
    "    # Recursively call the function for the next depth level\n",
    "    return get_subpage_links(l, max_depth, current_depth + 1, write_frequency, csv_filename)\n",
    "\n",
    "def write_links_to_csv(links_dict, csv_filename):\n",
    "    \"\"\"Writes the current state of the links dictionary to a CSV file.\"\"\"\n",
    "    with open(csv_filename, \"w\", newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        for link in links_dict.keys():\n",
    "            csvwriter.writerow([link])\n",
    "\n",
    "    print(f\"Links saved to {csv_filename}.\")\n"
   ],
   "id": "340ba3461c63f04a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Getting the links from the sitemaps\n",
   "id": "edc385befce0d660"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we gather all the links from some pages\n",
    "\n",
    "# we test with the first page inside the final_links csv\n",
    "\n",
    "websites = sitemaps[1:]  # exclude the first row\n",
    "# create dictionary of website\n",
    "dict_links = { website : \"Not-checked\" for website in websites }\n",
    "\n",
    "counter, counter2 = None, 0\n",
    "csv_filename = \"link_data.csv\"\n",
    "\n",
    "while counter != 0:\n",
    "    counter2 += 1\n",
    "    dict_links2 = get_subpage_links(dict_links, csv_filename=csv_filename)\n",
    "    counter = operator.countOf(dict_links2.values(), \"Not-checked\")  # Number of \"Not-checked\" links\n",
    "    \n",
    "    # Print some statements for debugging\n",
    "    print(\"\")\n",
    "    print(\"THIS IS LOOP ITERATION NUMBER\", counter2)\n",
    "    print(\"LENGTH OF DICTIONARY WITH LINKS =\", len(dict_links2))\n",
    "    print(\"NUMBER OF 'Not-checked' LINKS = \", counter)\n",
    "    print(\"\")\n",
    "    \n",
    "    dict_links = dict_links2\n",
    "    \n",
    "\n",
    "write_links_to_csv(dict_links, csv_filename)\n",
    "\n",
    "print(\"Links saved to link_data.csv.\")\n",
    "\n",
    "# removing any link that points to a sitemap\n",
    "\n"
   ],
   "id": "1d9434319c325b31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Removing the sitemap links from the csv file",
   "id": "39f6c9b2985b3b46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "links = []\n",
    "\n",
    "with open('link_data.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if '.xml' not in row[0]:\n",
    "            links.append(row[0])\n",
    "\n",
    "with open('link_data.csv', mode='w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    for link in links:\n",
    "        csv_writer.writerow([link])\n",
    "        \n",
    "print(\"Links saved to link_data.csv.\")"
   ],
   "id": "59753b24e3dcc7eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# DECIDED TO NOT USE THIS METHOD \n",
    "### Creating a new csv file with the links content \n",
    "\n",
    "First csv that will contain only raw html data (from h1, p tags etc.)\n",
    "- URL,Source,title,h1,h2,h3,p,span\n",
    "\n",
    "Second csv will contain the links will contain a more finallized version of the data\n",
    "- URL,Source,Product_Name,Description,Price"
   ],
   "id": "57fa53806b8a73e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# tags_to_extract = ['h1', 'p'] # change this in order to get information from different tags\n",
    "# \n",
    "# output_file = 'raw_content.csv'\n",
    "# \n",
    "# def extract_text_from_url(url):\n",
    "#     html_data = get_data(url) # this is \"None\" if the link is not accessible\n",
    "#     if html_data is None:\n",
    "#         return None\n",
    "#     soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "#     \n",
    "#     extracted_text = { tag: [] for tag in tags_to_extract }\n",
    "#     \n",
    "#     for tag in tags_to_extract:\n",
    "#         elements = soup.find_all(tag)\n",
    "#         for element in elements:\n",
    "#             text = element.get_text(strip=True)\n",
    "#             if text:\n",
    "#                 extracted_text[tag].append(text)\n",
    "#     for tag in extracted_text:\n",
    "#         extracted_text[tag] = ' ### '.join(extracted_text[tag])\n",
    "#         \n",
    "#     return extracted_text\n",
    "# \n",
    "# def process_url(url):\n",
    "#     extracted_text = extract_text_from_url(url)\n",
    "#     if extracted_text:\n",
    "#         row = [url, get_base_url(url)]  # Replace 'Source Name' with your actual source\n",
    "#         for tag in tags_to_extract:\n",
    "#             row.append(extracted_text.get(tag, ''))  # Append text for each tag\n",
    "#         return row\n",
    "#     return None\n",
    "# \n",
    "# def save_text_to_csv(data):\n",
    "#     with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         headers = ['URL', 'Source'] + tags_to_extract\n",
    "#         writer.writerow(headers)\n",
    "#         for row in data:\n",
    "#             if row[2] and row[3]:\n",
    "#                 writer.writerow(row)"
   ],
   "id": "560a06202edd9f12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extracting strictly raw contents from websites and loading them to a csv file",
   "id": "437fe43abcd33be9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    # Replace multiple spaces and line breaks with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def extract_data(url):\n",
    "    html_data = get_data(url)\n",
    "    if html_data is None:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    # Finding the title in the meta tags\n",
    "    title = soup.find('title')\n",
    "    if title:\n",
    "        title = title.get_text()\n",
    "    else: return None\n",
    "        \n",
    "    # Remove scripts, styles, and irrelevant content\n",
    "    for script in soup([\"script\", \"style\", \"footer\", \"nav\", \"header\", \"noscript\"]):\n",
    "        script.extract()\n",
    "\n",
    "    h1_tags = [clean_text(h1.get_text()) for h1 in soup.find_all('h1')]\n",
    "    \n",
    "    page_text = clean_text(soup.get_text(separator=' '))\n",
    "\n",
    "    # If no h1 tags are found, return None\n",
    "    if not h1_tags:\n",
    "        return None\n",
    "\n",
    "    # Only take the first h1 tag and its position\n",
    "    h1_tag = h1_tags[0]  \n",
    "    \n",
    "    # We get the last path of the url\n",
    "    url_index = url.rfind('/')\n",
    "    url_last_path = url[url_index + 1:].replace('-', ' ') # Get the last part of th\n",
    "    \n",
    "    if url_last_path:\n",
    "        return h1_tag, title, url_last_path, page_text\n",
    "    \n",
    "    return None\n",
    "    \n",
    "\n",
    "final_data = []\n",
    "links = links[:30000]  \n",
    "\n",
    "final_data_lock = threading.Lock()  # A lock to ensure thread-safe appending to the list\n",
    "\n",
    "# Function to process a single URL\n",
    "def process_url(link):\n",
    "    try:\n",
    "        h1_tag, title, url_last_path, page_text = extract_data(link)\n",
    "        with final_data_lock:  # Ensure thread-safe access to final_data\n",
    "            final_data.append([link, h1_tag, title, url_last_path, page_text])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {link}: {e}\")\n",
    "\n",
    "# Read from CSV and process in parallel\n",
    "with open('link_data.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:  # Adjust max_workers as needed\n",
    "        futures = []\n",
    "        for link in links:\n",
    "            futures.append(executor.submit(process_url, link))  # Submit tasks to the thread pool\n",
    "\n",
    "        # Track progress with tqdm\n",
    "        for _ in tqdm(concurrent.futures.as_completed(futures), total=len(links), desc=\"Processing URLs\"):\n",
    "            pass  # We don't need the result here since we're appending directly to final_data\n",
    "\n",
    "                          \n",
    "with open('data/preprocessed_data_from_all_sitemaps.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    csv_writer.writerow(['URL', 'h1', 'title', 'url_last_path', 'page_text'])\n",
    "    for row in final_data:\n",
    "        csv_writer.writerow(row)\n",
    "        \n",
    "\n",
    "print(\"Processing completed!\")\n",
    "\n"
   ],
   "id": "581f98826fea5713",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Extracting all relevant text from the links and maintaining its structure\n",
    "\n",
    "In this code block I will take a different approach and get all relevant data from the page and not altering its ordering (like I did above by separating the tags content apart). If this proves inefficient I will go back to the previous method but this makes more sense logically as long as the text segments are short enough for the model to understand, yet not to small for the model to not be able to understand the context (even though the context is some random text from the page like hyperlink text etc.)"
   ],
   "id": "cb886e2cb6e99b70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def similarity(spacy_doc1, spacy_doc2):\n",
    "    return spacy_doc1.similarity(spacy_doc2)\n",
    "\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    html_data = get_data(url)\n",
    "    if html_data is None:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    # Finding the title in the meta tags\n",
    "    title = soup.find('title')\n",
    "    if title:\n",
    "        title = title.get_text()\n",
    "        \n",
    "\n",
    "    # Remove scripts, styles, and irrelevant content\n",
    "    for script in soup([\"script\", \"style\", \"footer\", \"nav\", \"header\", \"noscript\"]):\n",
    "        script.extract()\n",
    "\n",
    "    h1_tags = [clean_text(h1.get_text()) for h1 in soup.find_all('h1')]\n",
    "    \n",
    "    page_text = clean_text(soup.get_text(separator=' '))\n",
    "\n",
    "    # If no h1 tags are found, return None\n",
    "    if not h1_tags:\n",
    "        return None\n",
    "\n",
    "    # Only take the first h1 tag and its position\n",
    "    h1_tag = h1_tags[0]  \n",
    "    \n",
    "    # We get the last path of the url\n",
    "    url_index = url.rfind('/')\n",
    "    url_last_path = url[url_index + 1:].replace('-', ' ') # Get the last part of the URL\n",
    "    \n",
    "    ################################################################################\n",
    "    # We check for similarity between the h1 tag and the last part of the URL\n",
    "    h1_tag_similarity = nlp(h1_tag)\n",
    "    url_last_path_similarity = nlp(url_last_path)\n",
    "    title_similarity = nlp(title)\n",
    "    \n",
    "    sim1 = similarity(h1_tag_similarity, url_last_path_similarity)\n",
    "    sim2 = similarity(h1_tag_similarity, title_similarity)\n",
    "    sim3 = similarity(url_last_path_similarity, title_similarity)\n",
    "    \n",
    "    if sim1 < 0.65 or sim2 < 0.65 or sim3 < 0.65:\n",
    "        return None\n",
    "    \n",
    "    if h1_tag in page_text:\n",
    "        start_idx = page_text.index(h1_tag)\n",
    "        end_idx = start_idx + len(h1_tag)\n",
    "        h1_tag_position = (h1_tag, start_idx, end_idx)\n",
    "        return h1_tag_position, title, url_last_path, page_text\n",
    "    return None\n",
    "\n",
    "def tokenize_and_label(text, h1_tag_position, title, url_last_path, token_window=30):\n",
    "    tokens = text.split()\n",
    "    labels = ['O'] * len(tokens)  # Default all tokens to 'O'\n",
    "    \n",
    "    # Create character-to-token index mapping\n",
    "    char_to_token_idx = []\n",
    "    current_pos = 0\n",
    "    for token in tokens:\n",
    "        char_to_token_idx.append(current_pos)\n",
    "        current_pos += len(token) + 1  # Adding 1 for the space separator\n",
    "    \n",
    "    # We only care about the first h1_tag position\n",
    "    h1_text, start_idx, end_idx = h1_tag_position\n",
    "\n",
    "    # Find the token indices corresponding to the h1_tag\n",
    "    start_token_idx = None\n",
    "    end_token_idx = None\n",
    "    for i, char_idx in enumerate(char_to_token_idx):\n",
    "        if start_token_idx is None and char_idx >= start_idx:\n",
    "            start_token_idx = i\n",
    "        if char_idx > end_idx:\n",
    "            end_token_idx = i\n",
    "            break\n",
    "\n",
    "    if start_token_idx is not None:\n",
    "        # Label the h1 tag tokens\n",
    "        labels[start_token_idx] = 'B-PRODUCT'\n",
    "        if end_token_idx is None:  # If the tag is the last part of the text\n",
    "            end_token_idx = len(tokens)\n",
    "        for j in range(start_token_idx + 1, end_token_idx):\n",
    "            labels[j] = 'I-PRODUCT'\n",
    "\n",
    "        # Trim the tokens to keep a window around the product\n",
    "        start_window = max(0, start_token_idx - token_window)\n",
    "        end_window = min(len(tokens), end_token_idx + token_window)\n",
    "        \n",
    "        # Trim the tokens and labels to the window\n",
    "        tokens = tokens[start_window:end_window]\n",
    "        labels = labels[start_window:end_window]\n",
    "        \n",
    "        # [URL] oak chair [URL] [TITLE] Oak Chair made by XYZ [TITLE] [TEXT] Oak Chair made by XYZ, Price: 40 euro etc [TEXT]\n",
    "        \n",
    "        # Add special tokens for the URL, title, and text\n",
    "        url_last_path = url_last_path.split()\n",
    "        title = title.split()\n",
    "        tokens = ['[URL]'] + url_last_path + ['[URL]', '[TITLE]'] + title + ['[TITLE]', '[TEXT]'] + tokens + ['[TEXT]']\n",
    "        \n",
    "        # Adjust the labels to match the new tokens\n",
    "        labels = ['O'] + ['B-PROD'] + ['I-PROD'] * len(url_last_path) + ['O', 'O'] + ['B-PROD'] + ['I-PROD'] * len(title) + ['O', 'O'] + labels + ['O'] # this will be left as it is since we are considering that there are urls and titles inside the text\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "# Process a single URL and return a row with extracted text\n",
    "def process_url(url): # THIS METHOD is where I was left yesterday\n",
    "    try:\n",
    "        result = extract_text_from_url(url)\n",
    "        if result is None:\n",
    "            return None  # Skip processing if extraction failed\n",
    "\n",
    "        h1_tag_position, title, url_last_path, page_text = result\n",
    "\n",
    "        if page_text:\n",
    "            tokens, labels = tokenize_and_label(page_text, h1_tag_position, title, url_last_path)\n",
    "            return [url, get_base_url(url), tokens, labels]\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Save extracted text data into a CSV file\n",
    "import pandas as pd\n",
    "\n",
    "def save_text_to_csv(data, output_file='tokenized_data.csv'):\n",
    "    rows = []\n",
    "    for url, source, tokens, labels in data:\n",
    "        rows.append([url, source, tokens, labels])\n",
    "\n",
    "    # Create a DataFrame and save it to a CSV\n",
    "    df = pd.DataFrame(rows, columns=['URL', 'Source', 'Tokens', 'Labels'])\n",
    "    \n",
    "    # Save DataFrame to a CSV file\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"Data saved to {output_file}\")\n"
   ],
   "id": "7afeda848fbf46de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# GETTING THE LINKS FROM 'link_data.csv'\n",
    "\n",
    "urls = []\n",
    "\n",
    "with open('link_data.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        urls.append(row[0])\n",
    "        \n",
    "# import random\n",
    "# random.shuffle(urls)\n",
    "\n",
    "print(urls[:1000])\n",
    "\n"
   ],
   "id": "a3eb5b03bb419dee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# chat gpt made this method\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to extract the base URL (domain) from a full URL\n",
    "# Read all URLs from the CSV file\n",
    "urls = []\n",
    "with open('link_data.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        urls.append(row[0])\n",
    "\n",
    "# Group URLs by their base URL\n",
    "url_groups = defaultdict(list)\n",
    "for url in urls:\n",
    "    base_url = get_base_url(url)\n",
    "    url_groups[base_url].append(url)\n",
    "\n",
    "# Create a round-robin iterator of the base URL groups\n",
    "group_iterators = {base: itertools.cycle(group) for base, group in url_groups.items()}\n",
    "\n",
    "# Create a round-robin cycle of the base URLs\n",
    "base_urls = list(group_iterators.keys())\n",
    "round_robin_bases = itertools.cycle(base_urls)\n",
    "\n",
    "# Generate the dispersed list of URLs\n",
    "dispersed_urls = []\n",
    "used_urls = set()\n",
    "\n",
    "# Loop until all URLs are used\n",
    "while len(used_urls) < len(urls):\n",
    "    for base_url in base_urls:\n",
    "        # Get the next URL from the current base URL group\n",
    "        try:\n",
    "            next_url = next(group_iterators[base_url])\n",
    "            if next_url not in used_urls:\n",
    "                dispersed_urls.append(next_url)\n",
    "                used_urls.add(next_url)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "# Save the dispersed URLs back to a CSV file\n",
    "with open('dispersed_link_data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    for url in dispersed_urls:\n",
    "        csv_writer.writerow([url])\n",
    "\n",
    "print(f\"Dispersed {len(dispersed_urls)} URLs successfully!\")"
   ],
   "id": "599e2af6a49a89b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "extracted_data = []\n",
    "    \n",
    "\n",
    "with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "    future_to_url = {executor.submit(process_url, url): url for url in urls[:5000]}\n",
    "    for future in tqdm(concurrent.futures.as_completed(future_to_url), total=len(future_to_url), desc=\"Extracting content from URLs (p, h1, h2 etc.)\"): \n",
    "        result = future.result()\n",
    "        if result:\n",
    "            extracted_data.append(result)\n",
    "            \n",
    "print(extracted_data)\n"
   ],
   "id": "3ea0363b4856970a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the extracted content to a CSV file\n",
    "\n",
    "\n",
    "\n",
    "save_text_to_csv(extracted_data, output_file='Data/dataset_for_training_1.csv')"
   ],
   "id": "14e35c0ca21c7454",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = []\n",
    "counter = 0\n",
    "\n",
    "\n",
    "# Increase the field size limit\n",
    "csv.field_size_limit(10**7)\n",
    "\n",
    "\n",
    "with open('Data/dataset_for_training_1.csv', mode='r', newline='', encoding='utf-8') as file:  # 46986\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        counter += 1\n",
    "        data.append([row[2]])\n",
    "        \n",
    "\n",
    "with open('Data/dataset_for_training_1.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    for row in data:\n",
    "        row = row[0].replace('[', '').replace(']', '').replace('\\'', '').replace(',', '').replace('\\\"', '')\n",
    "        csv_writer.writerow([row])\n",
    "\n"
   ],
   "id": "d0ecc46f4aa812a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "65412ca13beb6924"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(data[200][0], data[200][2])",
   "id": "ead6f4b84e820a87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(len(extracted_data))",
   "id": "814a65d249e0e242",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = 'https://www.ikea.com/us/en/p/brimnes-bed-frame-w-storage-and-headboard-white-luroey-s69216757/'\n",
    "\n",
    "html_data = extract_text_from_url(url)\n",
    "\n",
    "print(html_data[0])\n",
    "\n",
    "print(html_data[1][1027:1091])"
   ],
   "id": "44ff3acbf690358e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(len(extracted_data))",
   "id": "49a90453d27666a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test to see how many links have titles",
   "id": "25a9299e4b1a5347"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fetch_title(url):\n",
    "    try:\n",
    "        data = get_data(url)  # Assuming get_data is defined elsewhere\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        title_tag = soup.find('title')\n",
    "        if title_tag:\n",
    "            return title_tag.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Read URLs from the CSV file and process them with multithreading\n",
    "titles = []\n",
    "urls = []\n",
    "\n",
    "# Load URLs from CSV file\n",
    "with open('Data/furniture stores pages.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        urls.append(row[0])\n",
    "\n",
    "# Use ThreadPoolExecutor for multithreading\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:  # Adjust the number of workers based on your CPU\n",
    "    future_to_url = {executor.submit(fetch_title, url): url for url in urls}\n",
    "\n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        url = future_to_url[future]\n",
    "        try:\n",
    "            title = future.result()\n",
    "            if title:\n",
    "                titles.append(title)\n",
    "        except Exception as exc:\n",
    "            print(f\"{url} generated an exception: {exc}\")\n",
    "\n",
    "# Print the collected titles\n",
    "print(titles, len(titles))\n"
   ],
   "id": "8cd191523d593ae8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "unique_base_urls = set()\n",
    "\n",
    "\n",
    "with open('link_data.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        base_url = get_base_url(row[0])\n",
    "        unique_base_urls.add(base_url)\n",
    "        \n",
    "        \n",
    "print(len(unique_base_urls))\n"
   ],
   "id": "4b0becdd6041e366",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "737ee3c6c0e77bf4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
