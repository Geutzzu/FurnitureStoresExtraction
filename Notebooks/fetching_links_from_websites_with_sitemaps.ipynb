{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Web scraping the sites that contain site maps\n",
    "\n",
    "This notebook aims to gather all the data from multiple websites that contain site maps. The target is to get as many product websites from different pages with as many different furniture types as possible."
   ],
   "id": "7c6a352b9f6cb4a9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# importing libraries\n",
    "\n",
    "import csv \n",
    "import operator\n",
    "import re\n",
    "import threading\n",
    "\n",
    "import sys\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import spacy # we use this for word similarity\n",
    "\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Methods for getting the base url and for checking if the website has a sitemap",
   "id": "82c89620a1731cc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "SITEMAP_PATHS = [\n",
    "    \"sitemap.xml\",\n",
    "    \"sitemap_index.xml\",\n",
    "    \".sitemap.xml\",\n",
    "    \"sitemap/sitemap.xml\",\n",
    "    \"sitemap_index/sitemap.xml\"\n",
    "]\n",
    "\n",
    "def get_base_url(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        return base_url\n",
    "    except Exception as e:\n",
    "        # print(f\"Error parsing URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_sitemap(base_url):\n",
    "    \"\"\"\n",
    "    Check if the given base URL has a sitemap in one of the common paths.\n",
    "    Returns the URL of the sitemap if found, otherwise None.\n",
    "    \"\"\"\n",
    "    for sitemap_path in SITEMAP_PATHS:\n",
    "        sitemap_url = urljoin(base_url, sitemap_path)\n",
    "        try:\n",
    "            response = requests.head(sitemap_url, timeout=10)\n",
    "            # Check if the URL exists and returns a successful status code (200)\n",
    "            if 200 <= response.status_code < 300:\n",
    "                return sitemap_url\n",
    "        except requests.RequestException as e:\n",
    "            # print(f\"Error checking {sitemap_url}: {e}\")\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def check_sitemap_concurrently(base_urls):\n",
    "    \"\"\"\n",
    "    Checks sitemaps for a list of base URLs concurrently.\n",
    "    Returns a list of results with the base URL and sitemap URL.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        futures = {executor.submit(check_sitemap, base_url): base_url for base_url in base_urls}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Checking sitemaps\"):\n",
    "            base_url = futures[future]\n",
    "            sitemap_url = future.result()\n",
    "            if sitemap_url:\n",
    "                results.append([base_url, \"Sitemap found\", sitemap_url])\n",
    "            else:\n",
    "                results.append([base_url, \"No sitemap found\", \"\"])\n",
    "    return results"
   ],
   "id": "be2f786884fe403e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filtering out the websites that are not accessible that have a site map",
   "id": "f654312a4e74e8f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# reading the csv file and storing the links in the links list\n",
    "links = []\n",
    "with open('Data/furniture stores pages.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        try:\n",
    "            link = get_base_url(row[0])\n",
    "            links.append(link)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "# Check sitemaps concurrently\n",
    "sitemap_results = check_sitemap_concurrently(links)\n",
    "\n",
    "# Write the results to the output CSV file\n",
    "output_csv = \"sitemap_results.csv\"\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([\"URL\", \"Status\", \"Sitemap URL\"])  # Header row\n",
    "    csvwriter.writerows(sitemap_results)  # Write all results\n",
    "\n",
    "print(f\"Results saved to {output_csv}.\")"
   ],
   "id": "cbdd1c3171951b50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Getting the sitemaps from the csv file",
   "id": "53637297a4e4279d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# getting only the sitemaps from the csv file\n",
    "sitemaps = []\n",
    "with open('sitemap_results.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if row[1] != 'No sitemap found':\n",
    "            sitemaps.append(row[2])\n",
    "\n",
    "print(sitemaps[1:], len(sitemaps))"
   ],
   "id": "ddab114938e8c622",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Methods for getting the links from the sitemaps",
   "id": "7212ef201c988cf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_data(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "    return response.text\n",
    "\n",
    "def is_valid_product_link(url):\n",
    "    # Exclude common unwanted patterns\n",
    "    unwanted_patterns = ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.css', '.js', '.ico', 'tel:', 'mailto:', '#']\n",
    "    for pattern in unwanted_patterns:\n",
    "        if pattern in url:\n",
    "            return False\n",
    "    # Only accept URLs that contain \"collections\" or \"products\"\n",
    "    if '/products/' in url: # or  'collections' in url: - right now I will only focus on the products path\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_desired_site_map_link(url): #  \n",
    "    # Only accept URLs that contain \"sitemap\"\n",
    "    if 'sitemap_products_1.xml' in url: # most website have this in the url that I am looking for\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "dict_href_links = {}\n",
    "\n",
    "def get_links_from_sitemap(website_link): # modified version from the one in the other notebook\n",
    "    # Set the base of the URL depending on whether \"collections\" or \"products\" is in the link\n",
    "    website_origin = website_link\n",
    "    sitemap_index = website_link.find('/sitemap')\n",
    "    if sitemap_index == -1:\n",
    "        sitemap_index = website_link.find('/.sitemap')\n",
    "    if sitemap_index != -1:\n",
    "        website_origin = website_link[:sitemap_index + 1] \n",
    "\n",
    "    html_data = get_data(website_link)\n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    list_links = []\n",
    "\n",
    "    for link in soup.find_all(\"loc\"): # this contains the links inside xml files\n",
    "        link = link.text\n",
    "        # Filter out invalid links (non-product/collection pages)\n",
    "        \n",
    "        if not is_valid_product_link(link) and not is_desired_site_map_link(link): \n",
    "            continue\n",
    "        \n",
    "        link_to_append = None\n",
    "\n",
    "        # Handle absolute URLs that start with the origin\n",
    "        if link.startswith(str(website_origin)):\n",
    "            link_to_append = link\n",
    "        \n",
    "        # Handle relative URLs that start with \"/\"\n",
    "        elif link.startswith(\"/\"):\n",
    "            #print(href)\n",
    "            link_with_www = website_origin + link[1:]\n",
    "            #print(\"adjusted link =\", link_with_www)\n",
    "            link_to_append = link_with_www\n",
    "        \n",
    "\n",
    "        \n",
    "        # If link_to_append is not None, check if it's already in dict_href_links and if it's accessible\n",
    "        if link_to_append is not None:\n",
    "            if link_to_append not in dict_href_links: #  and check_website(link_to_append) - I will not check the links here, I will check them after I get all the links\n",
    "                dict_href_links[link_to_append] = None  # Mark it as seen\n",
    "\n",
    "                list_links.append(link_to_append)\n",
    "\n",
    "    # Convert list of links to a dictionary with \"Not-checked\" as the default value for each\n",
    "    dict_links = dict.fromkeys(list_links, \"Not-checked\")\n",
    "    return dict_links\n",
    "\n",
    "def get_subpage_links(l, max_depth=3, current_depth=0, write_frequency=500, csv_filename=\"link_data.csv\"):\n",
    "    processed_links_count = 0\n",
    "    \n",
    "    if current_depth >= max_depth:\n",
    "        return l\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        futures = {executor.submit(get_links_from_sitemap, link): link for link in l if l[link] == \"Not-checked\"}\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing subpage links\"):\n",
    "            link = futures[future]\n",
    "            try:\n",
    "                dict_links_subpages = future.result()\n",
    "                # print(f\"Processed {link} with {len(dict_links_subpages)} subpages.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                continue\n",
    "            l[link] = \"Checked\"\n",
    "            l.update(dict_links_subpages)\n",
    "\n",
    "            processed_links_count += 1\n",
    "            \n",
    "            # Write to file every 'write_frequency' processed links\n",
    "            if processed_links_count >= write_frequency: # this actually writes all the links to the csv file - even the not checked ones but in my case it is sufficient\n",
    "                write_links_to_csv(l, csv_filename)\n",
    "                processed_links_count = 0  # Reset the counter\n",
    "\n",
    "    # Recursively call the function for the next depth level\n",
    "    return get_subpage_links(l, max_depth, current_depth + 1, write_frequency, csv_filename)\n",
    "\n",
    "def write_links_to_csv(links_dict, csv_filename):\n",
    "    \"\"\"Writes the current state of the links dictionary to a CSV file.\"\"\"\n",
    "    with open(csv_filename, \"w\", newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        for link in links_dict.keys():\n",
    "            csvwriter.writerow([link])\n",
    "\n",
    "    print(f\"Links saved to {csv_filename}.\")\n"
   ],
   "id": "340ba3461c63f04a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Getting the links from the sitemaps\n",
   "id": "edc385befce0d660"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we gather all the links from some pages\n",
    "\n",
    "# we test with the first page inside the final_links csv\n",
    "\n",
    "websites = sitemaps[1:]  # exclude the first row\n",
    "# create dictionary of website\n",
    "dict_links = { website : \"Not-checked\" for website in websites }\n",
    "\n",
    "counter, counter2 = None, 0\n",
    "csv_filename = \"link_data.csv\"\n",
    "\n",
    "while counter != 0:\n",
    "    counter2 += 1\n",
    "    dict_links2 = get_subpage_links(dict_links, csv_filename=csv_filename)\n",
    "    counter = operator.countOf(dict_links2.values(), \"Not-checked\")  # Number of \"Not-checked\" links\n",
    "    \n",
    "    # Print some statements for debugging\n",
    "    print(\"\")\n",
    "    print(\"THIS IS LOOP ITERATION NUMBER\", counter2)\n",
    "    print(\"LENGTH OF DICTIONARY WITH LINKS =\", len(dict_links2))\n",
    "    print(\"NUMBER OF 'Not-checked' LINKS = \", counter)\n",
    "    print(\"\")\n",
    "    \n",
    "    dict_links = dict_links2\n",
    "    \n",
    "\n",
    "write_links_to_csv(dict_links, csv_filename)\n",
    "\n",
    "print(\"Links saved to link_data.csv.\")\n",
    "\n",
    "# removing any link that points to a sitemap\n",
    "\n"
   ],
   "id": "1d9434319c325b31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Removing the sitemap links from the csv file",
   "id": "39f6c9b2985b3b46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "links = []\n",
    "\n",
    "with open('link_data.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if '.xml' not in row[0]:\n",
    "            links.append(row[0])\n",
    "\n",
    "with open('link_data.csv', mode='w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    for link in links:\n",
    "        csv_writer.writerow([link])\n",
    "        \n",
    "print(\"Links saved to link_data.csv.\")"
   ],
   "id": "59753b24e3dcc7eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dispersing the links",
   "id": "f397a1c4a2ea47b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# GETTING THE LINKS FROM 'link_data.csv'\n",
    "\n",
    "urls = []\n",
    "\n",
    "with open('link_data.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        urls.append(row[0])\n",
    "        \n",
    "# import random\n",
    "# random.shuffle(urls)\n",
    "\n",
    "print(urls[:1000])\n",
    "\n"
   ],
   "id": "a3eb5b03bb419dee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Chat gpt made this method - I will not pretend to understand how it works but it does\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to extract the base URL (domain) from a full URL\n",
    "# Read all URLs from the CSV file\n",
    "urls = []\n",
    "with open('link_data.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        urls.append(row[0])\n",
    "\n",
    "# Group URLs by their base URL\n",
    "url_groups = defaultdict(list)\n",
    "for url in urls:\n",
    "    base_url = get_base_url(url)\n",
    "    url_groups[base_url].append(url)\n",
    "\n",
    "# Create a round-robin iterator of the base URL groups\n",
    "group_iterators = {base: itertools.cycle(group) for base, group in url_groups.items()}\n",
    "\n",
    "# Create a round-robin cycle of the base URLs\n",
    "base_urls = list(group_iterators.keys())\n",
    "round_robin_bases = itertools.cycle(base_urls)\n",
    "\n",
    "# Generate the dispersed list of URLs\n",
    "dispersed_urls = []\n",
    "used_urls = set()\n",
    "\n",
    "# Loop until all URLs are used\n",
    "while len(used_urls) < len(urls):\n",
    "    for base_url in base_urls:\n",
    "        # Get the next URL from the current base URL group\n",
    "        try:\n",
    "            next_url = next(group_iterators[base_url])\n",
    "            if next_url not in used_urls:\n",
    "                dispersed_urls.append(next_url)\n",
    "                used_urls.add(next_url)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "# Save the dispersed URLs back to a CSV file\n",
    "with open('dispersed_link_data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    for url in dispersed_urls:\n",
    "        csv_writer.writerow([url])\n",
    "\n",
    "print(f\"Dispersed {len(dispersed_urls)} URLs successfully!\")"
   ],
   "id": "599e2af6a49a89b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test to see how many links have titles (not that relevant):",
   "id": "25a9299e4b1a5347"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fetch_title(url):\n",
    "    try:\n",
    "        data = get_data(url)  # Assuming get_data is defined elsewhere\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        title_tag = soup.find('title')\n",
    "        if title_tag:\n",
    "            return title_tag.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Read URLs from the CSV file and process them with multithreading\n",
    "titles = []\n",
    "urls = []\n",
    "\n",
    "# Load URLs from CSV file\n",
    "with open('Data/furniture stores pages.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        urls.append(row[0])\n",
    "\n",
    "# Use ThreadPoolExecutor for multithreading\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:  # Adjust the number of workers based on your CPU\n",
    "    future_to_url = {executor.submit(fetch_title, url): url for url in urls}\n",
    "\n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        url = future_to_url[future]\n",
    "        try:\n",
    "            title = future.result()\n",
    "            if title:\n",
    "                titles.append(title)\n",
    "        except Exception as exc:\n",
    "            print(f\"{url} generated an exception: {exc}\")\n",
    "\n",
    "# Print the collected titles\n",
    "print(titles, len(titles))\n"
   ],
   "id": "8cd191523d593ae8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
