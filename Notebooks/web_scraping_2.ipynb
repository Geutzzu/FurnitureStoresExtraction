{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Web scraping for most of the sitemap websites",
   "id": "e58d6fa6cae01bda"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-17T18:15:17.845410Z",
     "start_time": "2024-09-17T18:15:14.609228Z"
    }
   },
   "source": [
    "# importing libraries\n",
    "\n",
    "import csv \n",
    "import operator\n",
    "import re\n",
    "import threading\n",
    "\n",
    "import sys\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import spacy # we use this for word similarity\n",
    "\n",
    "from collections import defaultdict\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading the links for scraping",
   "id": "70014b6139f4992b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T18:15:20.755322Z",
     "start_time": "2024-09-17T18:15:20.511533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "links = []\n",
    "\n",
    "with open('../link_data.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        links.append(row[0])"
   ],
   "id": "b45c1a630cbb50ca",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Methods that will be used for scraping\n",
   "id": "7e730773ae77fcc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_data(url, retries=3):\n",
    "    headers = {\n",
    "        'User-Agent': ua.random,\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Connection': 'keep-alive'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx and 5xx)\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        if retries > 0:\n",
    "            time.sleep(2 ** (4 - retries))  # Exponential backoff\n",
    "            return get_data(url, retries - 1)\n",
    "        return None"
   ],
   "id": "da2a0de946c3761f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
