{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training notebook made after reconsidering some choices made in the first training notebook.\n",
   "id": "825f13d03af37184"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-22T20:53:27.532840Z",
     "start_time": "2024-09-22T20:53:27.528827Z"
    }
   },
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import csv\n",
    "from urllib.parse import urlparse"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:53:27.562668Z",
     "start_time": "2024-09-22T20:53:27.559077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "\n",
    "label_all_tokens = False\n",
    "label_map = {'O': 0, 'B-PRODUCT': 1, 'I-PRODUCT': 2} # bert expects labels to be in the form of integers\n",
    "reverse_label_map = {v: k for k, v in label_map.items()} # we will use this to convert the model's output back to the original labels ffor metrics"
   ],
   "id": "a917852054efe345",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### This method is made specifically for grouping the data based on the url (it was initially dispersed due to having a different goal before deciding otherwise)",
   "id": "280b5d62e3dac7aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:17:12.167489Z",
     "start_time": "2024-09-22T21:17:12.160192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_base_url(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        return base_url\n",
    "    except Exception as e:\n",
    "        # print(f\"Error parsing URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def read_csv_file_grouped_by_base_url(file_path):\n",
    "    data_by_url = {}\n",
    "    row_count = 0\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            # Skip the header\n",
    "            if row[0] == \"URL\":\n",
    "                continue\n",
    "\n",
    "            url, tokens_str, labels_str = row\n",
    "            tokens = tokens_str.split(' ')\n",
    "            labels = labels_str.split(' ')\n",
    "\n",
    "            base_url = get_base_url(url)  # Get base URL\n",
    "\n",
    "            # Group sentences and labels by base URL\n",
    "            if base_url not in data_by_url:\n",
    "                data_by_url[base_url] = {'sentences': [], 'labels': []}\n",
    "\n",
    "            data_by_url[base_url]['sentences'].append(tokens)\n",
    "            data_by_url[base_url]['labels'].append(labels)\n",
    "            \n",
    "            row_count += 1\n",
    "            if row_count >= 4000:\n",
    "                break\n",
    "\n",
    "    # Now we convert labels to integers\n",
    "    for base_url, data in data_by_url.items():\n",
    "        for i in range(len(data['labels'])):\n",
    "            data['labels'][i] = [label_map[label] for label in data['labels'][i]]\n",
    "\n",
    "    return data_by_url"
   ],
   "id": "a50d8521eb281cfc",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:17:26.755389Z",
     "start_time": "2024-09-22T21:17:26.504687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and group your data by base URL\n",
    "data_by_url = read_csv_file_grouped_by_base_url(\"../data/100000_data_ready_for_training.csv\")\n",
    "\n",
    "# Get the list of unique base URLs\n",
    "base_urls = list(data_by_url.keys())\n",
    "\n",
    "# Perform the train/test split on the base URLs (instead of individual entries)\n",
    "train_urls, test_urls = train_test_split(base_urls, test_size=0.15, random_state=42)\n",
    "\n",
    "# Now split the data into train and test sets based on the base URLs\n",
    "train_sentences, train_labels = [], []\n",
    "test_sentences, test_labels = [], []\n",
    "\n",
    "for base_url in train_urls:\n",
    "    train_sentences.extend(data_by_url[base_url]['sentences'])\n",
    "    train_labels.extend(data_by_url[base_url]['labels'])\n",
    "\n",
    "for base_url in test_urls:\n",
    "    test_sentences.extend(data_by_url[base_url]['sentences'])\n",
    "    test_labels.extend(data_by_url[base_url]['labels'])\n",
    "\n",
    "print(f\"Training entries: {len(train_sentences)}, Testing entries: {len(test_sentences)}\")\n"
   ],
   "id": "2200bd0cc0c0f703",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 3410, Testing entries: 590\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:17:29.041837Z",
     "start_time": "2024-09-22T21:17:29.036936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(train_sentences, train_labels):\n",
    "    tokenized_inputs = tokenizer(train_sentences, truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(train_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # If this is the first token of a word, use the corresponding label\n",
    "            elif word_idx != previous_word_idx:\n",
    "                if word_idx < len(label):  # Check if the word index is within label range\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    # If the word index is out of range, append -100 (ignore token)\n",
    "                    label_ids.append(-100)\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ],
   "id": "c6973dabaaa86692",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:17:32.390138Z",
     "start_time": "2024-09-22T21:17:30.287776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "\n",
    "# We add the special tokens\n",
    "special_tokens = ['[URL]', '[TITLE]', '[TEXT]', '<NO_TITLE>', '<NO_URL>']\n",
    "\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "\n",
    "# tokenize and align labels for both training and test datasets\n",
    "train_data = tokenize_and_align_labels(train_sentences, train_labels)\n",
    "test_data = tokenize_and_align_labels(test_sentences, test_labels)\n",
    "\n",
    "# Convert the tokenized data to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)"
   ],
   "id": "f6e83b7dd7e348ec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boroz\\Documents\\GitHub\\FurnitureStoresExtraction\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(train_dataset[0])\n",
    "print(test_dataset[0])"
   ],
   "id": "24f264e27276274a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:17:35.309444Z",
     "start_time": "2024-09-22T21:17:33.308352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we load the model\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_map))\n",
    "model.resize_token_embeddings(len(tokenizer)) # This is done because of the special tokens we added\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-for-product-extraction\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [reverse_label_map[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [reverse_label_map[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))"
   ],
   "id": "5be267c2d23ecca2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\boroz\\Documents\\GitHub\\FurnitureStoresExtraction\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3410 590\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:23:58.360733Z",
     "start_time": "2024-09-22T21:17:37.314516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(trainer.evaluate())"
   ],
   "id": "4cd0b199a3da67bb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='642' max='642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [642/642 06:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.091426</td>\n",
       "      <td>0.573857</td>\n",
       "      <td>0.745139</td>\n",
       "      <td>0.648377</td>\n",
       "      <td>0.971562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.088747</td>\n",
       "      <td>0.627064</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.709986</td>\n",
       "      <td>0.973304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.109500</td>\n",
       "      <td>0.083623</td>\n",
       "      <td>0.639351</td>\n",
       "      <td>0.787178</td>\n",
       "      <td>0.705605</td>\n",
       "      <td>0.975437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37/37 00:06]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08362289518117905, 'eval_precision': 0.6393512590695689, 'eval_recall': 0.7871781397792958, 'eval_f1': 0.7056052755534621, 'eval_accuracy': 0.9754369083532716, 'eval_runtime': 7.1268, 'eval_samples_per_second': 82.786, 'eval_steps_per_second': 5.192, 'epoch': 3.0}\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "354f6c85aa161917"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
