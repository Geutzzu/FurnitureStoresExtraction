{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## In this notebook, all of the data preprocessing will be done.",
   "id": "ea28a462fd052f94"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-22T21:05:49.644503Z",
     "start_time": "2024-09-22T21:05:49.175170Z"
    }
   },
   "source": [
    " \n",
    "# importing libraries\n",
    "\n",
    "import csv\n",
    "csv.field_size_limit(5000000)\n",
    "import ast\n",
    "import operator\n",
    "import re\n",
    "import threading\n",
    "import random\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import spacy # we use this for word similarity\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### A little bit of data preprocessing (i.e. manually removing data that is likely wrong by removing entries with less that 1 words in the title, with strange symbols, etc.)\n",
    "By looking at the data manually I noticed what the wrong titles contain. The dataset will still be a bit noisy but I consider we have enough data to work with.\n",
    "At the bottom of the code cell you can also see the number of distinct websites in the dataset just so you can get an idea of the diversity of the dataset. When I wrote the code it was 288 / 705 (the initial dataset). This should be enough diversity for the model to learn from.\n"
   ],
   "id": "d8d6649d1af51537"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:06:12.083415Z",
     "start_time": "2024-09-22T21:05:52.892030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = []\n",
    "distinct_websites = set()\n",
    "\n",
    "BAD_TEXT_PATTERNS_IN_TITLE = ['releases', 'products', 'product', 'collections', 'collection', 'item', 'personalization', 'personalize', 'personalized', 'customize', 'customized', 'customise', 'customised', 'shop', 'store', 'stores', 'home', 'page', 'pages', 'about', 'contact', 'contact us', 'contact me', 'contact info']\n",
    "\n",
    "def clean_text(s):\n",
    "    # Define the pattern to allow only \"normal\" characters and keep relevant punctuation\n",
    "    allowed_pattern = r\"[^a-zA-Z0-9\\s,.:;\\'\\\"!?()\\-&+]\"\n",
    "\n",
    "    # Replace irrelevant characters with empty string (i.e., remove them)\n",
    "    return re.sub(allowed_pattern, '', s)\n",
    "\n",
    "def get_base_url(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        return base_url\n",
    "    except Exception as e:\n",
    "        # print(f\"Error parsing URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def literal_eval(item):\n",
    "    try:\n",
    "        # Check if the string looks like a tuple or list\n",
    "        if item.startswith('(') and item.endswith(')'):\n",
    "            return ast.literal_eval(item)\n",
    "        return None\n",
    "    except (ValueError, SyntaxError):\n",
    "        # Return the original string if evaluation fails\n",
    "        return None\n",
    "    \n",
    "def find_all_h1_positions(text, h1_tag): # original_position is a tuple (start, end)\n",
    "    positions = []\n",
    "    start_pos = 0\n",
    "\n",
    "    # Search for all occurrences of h1_tag in text\n",
    "    while True:\n",
    "        start_idx = text.find(h1_tag, start_pos)\n",
    "        \n",
    "        if start_idx == -1:\n",
    "            break\n",
    "        \n",
    "        end_idx = start_idx + len(h1_tag) \n",
    "        \n",
    "        positions.append((start_idx, end_idx))\n",
    "        \n",
    "        # Move start position forward to search for the next occurrence\n",
    "        start_pos = end_idx + 1\n",
    "\n",
    "    return h1_tag, positions\n",
    "\n",
    "\n",
    "with open('../data/preprocessed_data_from_all_sitemaps_100000.csv', 'r', encoding='utf-8', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        row[1] = literal_eval(row[1])\n",
    "        if row[1] is None:\n",
    "            continue\n",
    "        row[1] = (clean_text(row[1][0]), row[1][1], row[1][2]) # url, h1_tag_position, title, url_last_path, page_text = row\n",
    "        row[2] = clean_text(row[2])\n",
    "        row[3] = clean_text(row[3])\n",
    "        row[4] = clean_text(row[4])\n",
    "        \n",
    "        h1_tag, positions = find_all_h1_positions(row[4], row[1][0])\n",
    "        row[1] = (h1_tag, positions) # string with list\n",
    "        if row[1] is None or row[1][1] is None or 'G Plan Chloe' in row[1]: # this website haunts me in my data with G Plan Chloe\n",
    "            continue\n",
    "        \n",
    "        # finally we remove anything that has a len < 2\n",
    "        if len(row[1][0].split()) < 2:\n",
    "            continue\n",
    "        \n",
    "        ok = True\n",
    "        for word in row[1][0].split():\n",
    "            if word.lower() in BAD_TEXT_PATTERNS_IN_TITLE:\n",
    "                ok = False\n",
    "                break\n",
    "        if not ok:\n",
    "            continue    \n",
    "        \n",
    "        distinct_websites.add(get_base_url(row[0]))\n",
    "        data.append(row)\n",
    "        \n",
    "print(len(distinct_websites))"
   ],
   "id": "e7377e338b9504cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### This method will be used for labeling the data automatically\n",
    "\n",
    "I have decided to not label the title and the url last path in this version since they are quite inconsistent with the label I am looking for inside the text (i.e. the product name which is most of the time the h1 tag inside the text). That is what I consider the most accurate representation of the \"product name\" in the text."
   ],
   "id": "e10dfbdfc784ee84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:14:09.447366Z",
     "start_time": "2024-09-22T21:14:09.442826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def label_text(text, to_label):\n",
    "    # Split the main text and the text to be labeled into tokens\n",
    "    tokens = text.split()\n",
    "    label_tokens = to_label.split()\n",
    "\n",
    "    # Initialize the labels list with 'O' for 'Outside'\n",
    "    labels = ['O'] * len(tokens)\n",
    "\n",
    "    # Iterate through the tokens and label all occurrences of the label_tokens\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Convert both the token slice and label tokens to lowercase for comparison\n",
    "        if [t.lower() for t in tokens[i:i+len(label_tokens)]] == [lt.lower() for lt in label_tokens]:\n",
    "            labels[i] = 'B-PRODUCT'  # Mark the beginning of the label\n",
    "            for j in range(1, len(label_tokens)):\n",
    "                labels[i+j] = 'I-PRODUCT'  # Mark the rest of the label tokens\n",
    "            i += len(label_tokens)  # Skip the tokens that were labeled\n",
    "        else:\n",
    "            i += 1  # Move to the next token\n",
    "\n",
    "    return tokens, labels\n"
   ],
   "id": "feafc6cf62b8fa35",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:14:15.552063Z",
     "start_time": "2024-09-22T21:14:15.545068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_label(text, h1_tag_position, title, url_last_path, tokens_left=10, tokens_right=25):\n",
    "    \n",
    "    to_label = h1_tag_position[0]\n",
    "    \n",
    "    tokens_url_last_path, labels_url_last_path = [], []\n",
    "    tokens_title, labels_title = [], []\n",
    "    \n",
    "    if url_last_path is None or url_last_path == '':\n",
    "        tokens_url_last_path = ['<NO_URL>']\n",
    "        labels_url_last_path = ['O']\n",
    "    elif isinstance(url_last_path, str):\n",
    "        tokens_url_last_path, labels_url_last_path = label_text(url_last_path, to_label)\n",
    "    else: \n",
    "        tokens_url_last_path = ['<NO_URL>']\n",
    "        labels_url_last_path = ['O']\n",
    "\n",
    "    if title is None or title == '':\n",
    "        tokens_title = ['<NO_TITLE>']\n",
    "        labels_title = ['O']\n",
    "    elif isinstance(title, str):\n",
    "        tokens_title, labels_title = label_text(title, to_label)\n",
    "    else: \n",
    "        tokens_title = ['<NO_TITLE>']\n",
    "        labels_title = ['O']\n",
    "    \n",
    "    tokens_text, labels_text = label_text(text, to_label)\n",
    "    \n",
    "    try:\n",
    "        first_label_index = labels_text.index('B-PRODUCT')\n",
    "    except ValueError:\n",
    "        first_label_index = 0  # If no labeled entity, start from the beginning\n",
    "    try:\n",
    "        last_label_index = max(idx for idx, label in enumerate(labels_text) if label in ['B-PRODUCT', 'I-PRODUCT'])\n",
    "    except ValueError:\n",
    "        last_label_index = len(tokens_text) - 1  # If no labeled entity, end at the last token\n",
    "\n",
    "    # Calculate the window to slice\n",
    "    start_index = max(0, first_label_index - tokens_left)\n",
    "    end_index = min(len(tokens_text), last_label_index + tokens_right + 1) # or len(tokens_text) \n",
    "\n",
    "\n",
    "    tokens_text = tokens_text[start_index:end_index]\n",
    "    labels_text = labels_text[start_index:end_index]\n",
    "    \n",
    "    # tokens_text = tokens_text[:100]\n",
    "    # labels_text = labels_text[:100]\n",
    "    \n",
    "    tokens = ['[URL]'] + tokens_url_last_path + ['[URL]', '[TITLE]'] + tokens_title + ['[TITLE]', '[TEXT]'] + tokens_text + ['[TEXT]']\n",
    "    \n",
    "    labels = ['O'] + labels_url_last_path + ['O', 'O'] + labels_title + ['O', 'O'] + labels_text + ['O']\n",
    "    \n",
    "\n",
    "    return tokens, labels\n"
   ],
   "id": "9ae038adbc570abf",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:15:34.362430Z",
     "start_time": "2024-09-22T21:14:27.288168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../data/100000_data_ready_for_training.csv', 'w', encoding='utf-8', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for row in data:\n",
    "        url, h1_tag_positions, title, url_last_path, page_text = row\n",
    "        \n",
    "        tokens, labels = tokenize_and_label(page_text, h1_tag_positions, title, url_last_path)\n",
    "        # tokens, labels = clean_tokens_with_labels(tokens, labels)\n",
    "        tokens_str = ' '.join(tokens)  # Join tokens into a single string\n",
    "        labels_str = ' '.join(labels)  # Join labels into a single string\n",
    "        writer.writerow([url, tokens_str, labels_str])\n",
    "\n",
    "\n"
   ],
   "id": "7e402b5bfb985703",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:15:41.751558Z",
     "start_time": "2024-09-22T20:15:40.291018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessed_data = []\n",
    "\n",
    "with open('../data/100000_data_ready_for_training.csv', 'r', encoding='utf-8', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        url, tokens_str, labels_str = row\n",
    "        tokens = tokens_str.split(' ') # !!!\n",
    "        labels = labels_str.split(' ')\n",
    "        preprocessed_data.append((url, tokens, labels))"
   ],
   "id": "70e9a98d4e85b2c",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:16:00.707429Z",
     "start_time": "2024-09-22T20:16:00.704069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(data[9])\n",
    "print(preprocessed_data[9])\n"
   ],
   "id": "6739d2dcd05acdd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://mulamu.com/products/iedden-container', ('DDEN CONTAINER', [(1193, 1207), (1208, 1222)]), 'DDEN CONTAINER - Mulamu Furnishings', 'iedden container', 'MEMBERS ENJOY A 10 DISCOUNT! Be A Member Now Menu Cart FURNITURE ARMCHAIRS BED FRAMES Bed side tables BENCH COFFEE TABLES CONSOLES DINING CHAIRS DINING TABLES OUTDOOR FURNITURE STOOLS & BAR STOOLS STORAGE & SHELF STUDY DESKS SOFAS OTHERS customization order IN-STOCK BED ACCS  MATTRESS BED ACCESSORIES DOUBLE ELEPHANT MATTRESS Sofzsleep mattress OTHER DECOS LIGHTS  FANS ALL LIGHTING ALL FANS PENDANT LIGHTS FLOOR LAMP TABLE LAMP Sale Clearance SALE DISPLAY ASIS Furniture Rental OTHERS My Account Continue Shopping P.S. Would you like to purchase our lifetime membership and instantly enjoy 10 off your current cart and all future purchases? Click here to find out more! Your Cart is Empty MEMBERS ENJOY A 10 DISCOUNT! Be A Member Now FURNITURE  ARMCHAIRS BED FRAMES Bed side tables BENCH COFFEE TABLES CONSOLES DINING CHAIRS DINING TABLES OUTDOOR FURNITURE STOOLS & BAR STOOLS STORAGE & SHELF STUDY DESKS SOFAS OTHERS customization order IN-STOCK BED ACCS  MATTRESS  BED ACCESSORIES DOUBLE ELEPHANT MATTRESS Sofzsleep mattress OTHER DECOS LIGHTS  FANS  ALL LIGHTING ALL FANS PENDANT LIGHTS FLOOR LAMP TABLE LAMP Sale  Clearance SALE DISPLAY ASIS Furniture Rental OTHERS Cart Home  Products  DDEN CONTAINER DDEN CONTAINER 37.00 Notify me when this product is available: Made of natural solid wood and comes with a quirky crafted ceramic cover in a matt glaze finish. The perfect touch for any home or kitchen. Dimensions: Small: 120 x 120 x 200 mm Large: 120 x 120 x 300 mm Size S L Size S L Qty Add to Cart Share: You May Like These Too + Quick Shop BRK PLATE 25.00 BRK PLATE 25.00 Notify me when this product is available: Crafted with ceramic material in a matt glaze finish. Use this beautiful service ware to entertain your guests at home. Dimensions:300 x 300 x 30 mm View full product details C002 - OFFWHITE  LARGE Color C002 - OFFWHITE Size LARGE Qty Add to Cart + Quick Shop ELKING CUP Sold Out ELKING CUP Sold Out 6.00 Notify me when this product is available: A sophisticated ceramic mug in a matt glaze finish. Dimensions:120 x 86 x 105 mm View full product details + Quick Shop PBJLET PLATE 36.00 PBJLET PLATE 36.00 Notify me when this product is available: Crafted with ceramic in a matt glaze finish. You can use it to display your cakes or fruits. Dimensions:355 x 355 x 30 mm View full product details Color C001 - WHITE Color C001 - WHITE Qty Add to Cart + Quick Shop RPOSJO FRUIT PLATE from 19.00 RPOSJO FRUIT PLATE 19.00 Notify me when this product is available: Crafted with ceramic material in matt glaze finish. Simple but beautiful service ware to charm your guests. Dimensions:Small: 225 x 225 x 75 mm Small: 265 x 265 x 75 mm View full product details C001 - WHITE  S C002 - OFFWHITE  L C003 - BROWN  S Color C001 - WHITE C002 - OFFWHITE C003 - BROWN Size S L Qty Add to Cart Menu Mulamu  Home Blog FAQ Terms of Sales Terms of Use Contact Us Contact Us Whatsapp: 6341 6180 Email: salesmulamu.com Have a question? Send us a message! About Mulamu We are a team of quirky, enigmatic, outspoken and design-loving Singaporeans who share a love for high quality furnishings that boast unique designs. Our vision is to bring these features to everyone whilst doing so at competitive rates. Read more. News & Updates Sign up to get the latest on sales, new releases and more   2024 Mulamu Furnishings . All Rights Reserved. Be the first to know about our latest happenings We welcome you to join our mailing list to stay updated on our latest promotions...and enjoy exclusive discounts too! Powered By Discount Ninja  BODY TIMERFOOTER ACTIONBUTTON1LABEL ACTIONBUTTON2LABEL  HEADER MINIMIZED BODY FOOTER TIMERFOOTER Powered By Discount Ninja TEXT  Powered By Discount Ninja HEADER BODY FOOTER ACTIONBUTTON1LABEL ACTIONBUTTON2LABEL NOTHANKSBUTTONLABEL ACTIONBUTTON1LABEL ACTIONBUTTON2LABEL NOTHANKSBUTTONLABEL TEXT HEADER Powered By Discount Ninja BODY QUANTITYLABEL PRICELABEL TIERROW PRICE LABEL HEADER BODY FOOTER ROW']\n",
      "('https://mulamu.com/products/iedden-container', ['[URL]', 'iedden', 'container', '[URL]', '[TITLE]', 'DDEN', 'CONTAINER', '-', 'Mulamu', 'Furnishings', '[TITLE]', '[TEXT]', '[TEXT]'], ['O', 'O', 'O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'O', 'O', 'O', 'O', 'O'])\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fa436e10f66c83bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
