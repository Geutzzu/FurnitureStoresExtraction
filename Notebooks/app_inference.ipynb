{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import csv\n",
    "\n",
    "from Scripts.scraping import get_data\n",
    "\n",
    "csv.field_size_limit(5000000)\n",
    "import ast\n",
    "import operator\n",
    "import re\n",
    "\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:79.0) Gecko/20100101 Firefox/79.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "]\n",
    "\n",
    "\n",
    "# Function to get page content\n",
    "def get_data(url):\n",
    "    headers = {\"User-Agent\": random.choice(USER_AGENTS)}  # Rotate user-agent\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=3)\n",
    "\n",
    "        # Handle rate-limiting (HTTP 429) by pausing and retrying\n",
    "        if response.status_code == 429:\n",
    "            tqdm.write(f\"FROM GET_DATA: Rate limit reached. Sleeping before retrying {url}\")\n",
    "            # print(f\"FROM GET_DATA: Rate limit reached. Sleeping before retrying {url}\")\n",
    "            time.sleep(random.uniform(4, 8))  # Random delay to avoid detection\n",
    "            return get_data(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.content  # Return HTML content if successful\n",
    "\n",
    "        tqdm.write(f\"FROM GET_DATA: Failed to retrieve {url}, Status Code: {response.status_code}\")\n",
    "        # print(f\"FROM GET_DATA: Failed to retrieve {url}, Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        tqdm.write(f\"FROM GET_DATA: Error fetching {url}: {e}\")\n",
    "        # print(f\"FROM GET_DATA: Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def read_links_from_csv(csv_filename):\n",
    "    \"\"\"Writes the current state of the links dictionary to a CSV file.\"\"\"\n",
    "    links = {}\n",
    "    with open(csv_filename, \"r\", newline='') as csvfile:\n",
    "        csvwriter = csv.reader(csvfile)\n",
    "        for row in csvwriter:\n",
    "            links[row[0]] = ast.literal_eval(row[1])\n",
    "    return links\n"
   ],
   "id": "d788aeb77f719d79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def has_letters(input_string):\n",
    "    return any(char.isalpha() for char in input_string)\n",
    "\n",
    "def soup_mapper(soup):\n",
    "    word_tag_tuples = []\n",
    "    # Iterate through all tags\n",
    "    for tag in soup.descendants:\n",
    "        # Look only for deepest tags that dont contain other tags\n",
    "        if tag.name and not tag.find_all():\n",
    "            # Extract the word and tag\n",
    "            for word in tag.text.split():  \n",
    "                word_tag_tuples.append((word, tag))\n",
    "    \n",
    "    if len(word_tag_tuples) == 0:\n",
    "        return None\n",
    "    return word_tag_tuples\n",
    "\n",
    "def formated_link_content(link):\n",
    "    html_data = get_data(link)\n",
    "    if html_data is None:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    # Finding the title in the meta tags\n",
    "    title = soup.find('title')\n",
    "    if title:\n",
    "        title = title.get_text()\n",
    "    else: title = None\n",
    "    \n",
    "    # Remove scripts, styles, and irrelevant content\n",
    "    for script in soup([\"script\", \"style\", \"footer\", \"nav\", \"header\", \"noscript\", \"head\"]):\n",
    "        script.extract()\n",
    "\n",
    "    word_tag_tuples = soup_mapper(soup) # by joining word_tag_tuples[0] you get the full text\n",
    "    \n",
    "    url_index = link.rfind('/')\n",
    "    url_last_path = link[url_index + 1:].replace('-', ' ').replace('_', ' ')\n",
    "    \n",
    "    if not has_letters(url_last_path):\n",
    "        url_last_path = None\n",
    "    \n",
    "    return word_tag_tuples, title, url_last_path, soup\n",
    "\n",
    "\n",
    "formated_link_content(\"https://www.ikea.com/ro/ro/p/styrspel-scaun-de-gaming-purpuriu-negru-20522027/\")"
   ],
   "id": "2c33a86edce83cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3e8a4f0a707b51df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TESTS",
   "id": "cfbdb38f9ae5f403"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T23:15:33.374694Z",
     "start_time": "2024-09-25T23:15:33.370204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "html_data = \"<html><body><h1 class='something'>Title</h1><p>Food is cool</p><div>Another text</div></body></html>\"\n",
    "soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "\n",
    "word_tag_tuples = []\n",
    "\n",
    "# Iterate through all tags\n",
    "for tag in soup.descendants:\n",
    "    # Look only for deepest tags that dont contain other tags\n",
    "    if tag.name and not tag.find_all():\n",
    "        # Extract the word and tag\n",
    "        for word in tag.text.split():  \n",
    "            word_tag_tuples.append((word, tag))\n",
    "\n",
    "# Print the result\n",
    "for word, tag in word_tag_tuples:\n",
    "    print(f\"Word: {word}, Tag: {tag}\")\n",
    "    \n",
    "    \n",
    "print(soup.getText(separator=' '))"
   ],
   "id": "71662ad88060930d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Title, Tag: <h1 class=\"something\">Title</h1>\n",
      "Word: Food, Tag: <p>Food is cool</p>\n",
      "Word: is, Tag: <p>Food is cool</p>\n",
      "Word: cool, Tag: <p>Food is cool</p>\n",
      "Word: Another, Tag: <div>Another text</div>\n",
      "Word: text, Tag: <div>Another text</div>\n",
      "Title Food is cool Another text\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "387f755877dffa82"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
