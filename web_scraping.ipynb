{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Web Scraping and Data preprocessing tests and code attempts",
   "id": "f7fdf21e23770f91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T19:15:25.481597Z",
     "start_time": "2024-09-02T19:15:24.994194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import csv # importing the csv module\n",
    "import operator\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm # we use this to keep track of the progress of a loop\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n"
   ],
   "id": "f90e61da3ee0be17",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-02T18:12:20.738955Z",
     "start_time": "2024-09-02T18:12:20.733102Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# function to check if a website is accessible\n",
    "def check_website(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)  # timeout after 5 seconds\n",
    "        if 200 <= response.status_code < 300:\n",
    "            return True  # the website is accessible if the status code is between 200 and 299\n",
    "        else:\n",
    "            return False  # website returned an error (not accessible)\n",
    "    except requests.RequestException as e:\n",
    "        return False  # there was an issue with the request (e.g., domain error, timeout, etc.)\n",
    "\n",
    "def check_websites_concurrently(links):\n",
    "    accessible_links = []\n",
    "    with ThreadPoolExecutor(max_workers=30) as executor:\n",
    "        futures = {executor.submit(check_website, link): link for link in links}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(links), desc=\"Checking links\"):\n",
    "            # futures work like promises in JavaScript\n",
    "            link = futures[future] # get the link associated with the future from the futures dictionary (futures is a dictionary with the future as the key and the link as the value)\n",
    "            if future.result():\n",
    "                accessible_links.append(link)\n",
    "    return accessible_links\n",
    "\n",
    "\n",
    "\n",
    "def trim_url(url):\n",
    "    # find the first occurrence of \"collections\" and \"products\" in the URL\n",
    "    collections_index = url.find('/collections/')\n",
    "    products_index = url.find('/products/')\n",
    "    \n",
    "    # we trim the URL to the last \"/\" before \"collections\" or \"products\" using slicing\n",
    "    if collections_index != -1:\n",
    "        return url[:collections_index + len('/collections/')]\n",
    "    elif products_index != -1:\n",
    "        # by changing the products from the url with collections we get the page with the types of products and from my testing this approach proves best in order to access all the products from a webpage\n",
    "        # additionally, most websites have a collections path that points to the same place as the products path (don't know how this works, but I get over 400 working pages with this method which is way more than enough for training the model)\n",
    "        # throughout the code I will consider the products path or the collections path at the end of every URL (even though I may only keep one of them)\n",
    "        return url[:products_index] + '/collections/' # for having the products in the URL this must be changed with url[:products_index + len('/products/')]\n",
    "    else:\n",
    "        return url  # if neither \"collections\" nor \"products\" is found, we return the original URL\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# reading the csv file and storing the links in the links list\n",
    "links = []\n",
    "with open('Data/furniture stores pages.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        trimmed_url = trim_url(row[0])  # the trimmed url (the url is located in the first column)\n",
    "        links.append(trimmed_url) \n",
    "\n",
    "# we delete links that are not accessible\n",
    "final_links = check_websites_concurrently(links)\n",
    "\n",
    "# write the accessible links to a new csv file\n",
    "with open('final_links.csv', mode='w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    for link in final_links:\n",
    "        csv_writer.writerow([link])\n",
    "\n",
    "print(final_links, len(final_links))"
   ],
   "id": "cde434c02702a0e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T19:26:40.990646Z",
     "start_time": "2024-09-02T19:26:40.982624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# getting each pages links and going through them\n",
    "# major props to this medium post: https://python.plainenglish.io/scraping-the-subpages-on-a-website-ea2d4e3db113\n",
    "\n",
    "def get_data(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "def is_valid_product_link(url):\n",
    "    # Exclude common unwanted patterns\n",
    "    unwanted_patterns = ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.css', '.js', '.ico', 'tel:', 'mailto:', '#']\n",
    "    for pattern in unwanted_patterns:\n",
    "        if pattern in url:\n",
    "            return False\n",
    "    # Only accept URLs that contain \"collections\" or \"products\"\n",
    "    if '/collections/' in url or '/products/' in url:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "dict_href_links = {}\n",
    "\n",
    "def get_links(website_link):\n",
    "    # Set the base of the URL depending on whether \"collections\" or \"products\" is in the link\n",
    "    website_origin = website_link\n",
    "    collections_index = website_link.find('/collections/')\n",
    "    if collections_index != -1:\n",
    "        website_origin = website_link[:collections_index + 1] \n",
    "\n",
    "    html_data = get_data(website_link)\n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    list_links = []\n",
    "\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link[\"href\"]\n",
    "\n",
    "        # Filter out invalid links (non-product/collection pages)\n",
    "        if not is_valid_product_link(href):\n",
    "            continue\n",
    "        \n",
    "        link_to_append = None\n",
    "\n",
    "        # Handle absolute URLs that start with the origin\n",
    "        if href.startswith(str(website_origin)):\n",
    "            link_to_append = href\n",
    "        \n",
    "        # Handle relative URLs that start with \"/\"\n",
    "        elif href.startswith(\"/\"):\n",
    "            #print(href)\n",
    "            link_with_www = website_origin + href[1:]\n",
    "            #print(\"adjusted link =\", link_with_www)\n",
    "            link_to_append = link_with_www\n",
    "        \n",
    "        # If link_to_append is not None, check if it's already in dict_href_links and if it's accessible\n",
    "        if link_to_append is not None:\n",
    "            if link_to_append not in dict_href_links: #  and check_website(link_to_append) - I will not check the links here, I will check them after I get all the links\n",
    "                dict_href_links[link_to_append] = None  # Mark it as seen\n",
    "                list_links.append(link_to_append)\n",
    "\n",
    "    # Convert list of links to a dictionary with \"Not-checked\" as the default value for each\n",
    "    dict_links = dict.fromkeys(list_links, \"Not-checked\")\n",
    "    return dict_links\n",
    "\n",
    "def get_subpage_links(l, max_depth=3, current_depth=0):\n",
    "    if current_depth >= max_depth:\n",
    "        return l\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        futures = {executor.submit(get_links, link): link for link in l if l[link] == \"Not-checked\"}\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing subpage links\"):\n",
    "            link = futures[future]\n",
    "            try:\n",
    "                dict_links_subpages = future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                continue\n",
    "            l[link] = \"Checked\"\n",
    "            l.update(dict_links_subpages)\n",
    "\n",
    "    return get_subpage_links(l, max_depth, current_depth + 1)\n",
    "\n",
    "# old version of the function\n",
    "# def get_subpage_links(l, max_depth=3, current_depth=0):\n",
    "#     if current_depth >= max_depth:\n",
    "#         return l\n",
    "# \n",
    "#     for link in tqdm(l):\n",
    "#         if l[link] == \"Not-checked\":\n",
    "#             dict_links_subpages = get_links(link) \n",
    "#             l[link] = \"Checked\"\n",
    "#         else:\n",
    "#             dict_links_subpages = {}\n",
    "# \n",
    "#         l = {**dict_links_subpages, **l}\n",
    "#     \n",
    "#     return get_subpage_links(l, max_depth, current_depth + 1)\n"
   ],
   "id": "3191a80510d8b7f1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-02T19:14:30.171622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we gather all the links from some pages\n",
    "\n",
    "# we test with the first page inside the final_links csv\n",
    "\n",
    "website = \"https://www.factorybuys.com.au/collections/\"\n",
    "# create dictionary of website\n",
    "dict_links = {website:\"Not-checked\"}\n",
    "\n",
    "counter, counter2 = None, 0\n",
    "\n",
    "while counter != 0:\n",
    "    counter2 += 1\n",
    "    dict_links2 = get_subpage_links(dict_links)\n",
    "    counter = operator.countOf(dict_links2.values(), \"Not-checked\")  # Number of \"Not-checked\" links\n",
    "    \n",
    "    # Print some statements for debugging\n",
    "    print(\"\")\n",
    "    print(\"THIS IS LOOP ITERATION NUMBER\", counter2)\n",
    "    print(\"LENGTH OF DICTIONARY WITH LINKS =\", len(dict_links2))\n",
    "    print(\"NUMBER OF 'Not-checked' LINKS = \", counter)\n",
    "    print(\"\")\n",
    "    \n",
    "    dict_links = dict_links2\n",
    "\n",
    "# Write only the links to a CSV file after the loop completes\n",
    "with open(\"link_dat.csv\", \"w\", newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    # Write each link as a new row in the CSV file\n",
    "    for link in dict_links.keys():\n",
    "        csvwriter.writerow([link])\n",
    "\n",
    "print(\"Links saved to link_data.csv.\")"
   ],
   "id": "de2d374f49418f52",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subpage links: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Processing subpage links:   7%|▋         | 40/579 [00:44<10:05,  1.12s/it]\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
