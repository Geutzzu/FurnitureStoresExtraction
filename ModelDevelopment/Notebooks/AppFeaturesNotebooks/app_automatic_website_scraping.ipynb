{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## This will represent an option for my app when it comes to finding products on subpages",
   "id": "8f35fcc0bcb13820"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# importing libraries\n",
    "\n",
    "import csv \n",
    "import operator\n",
    "import re\n",
    "import threading\n",
    "\n",
    "import sys\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import spacy # we use this for word similarity\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_base_url(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        return base_url\n",
    "    except Exception as e:\n",
    "        # print(f\"Error parsing URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:79.0) Gecko/20100101 Firefox/79.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "]    \n",
    "\n",
    "def get_data(url):\n",
    "    headers = {\"User-Agent\": random.choice(USER_AGENTS)}  # Rotate user-agent\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=5)\n",
    "\n",
    "        # Handle rate-limiting (HTTP 429) by pausing and retrying\n",
    "        if response.status_code == 429:\n",
    "            tqdm.write(f\"FROM GET_DATA: Rate limit reached. Sleeping before retrying {url}\")\n",
    "            # print(f\"FROM GET_DATA: Rate limit reached. Sleeping before retrying {url}\")\n",
    "            time.sleep(random.uniform(4, 8))  # Random delay to avoid detection\n",
    "            return get_data(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.content  # Return HTML content if successful\n",
    "        \n",
    "        tqdm.write(f\"FROM GET_DATA: Failed to retrieve {url}, Status Code: {response.status_code}\")\n",
    "        \n",
    "        # print(f\"FROM GET_DATA: Failed to retrieve {url}, Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        tqdm.write(f\"FROM GET_DATA: Error fetching {url}: {e}\")\n",
    "        # print(f\"FROM GET_DATA: Error fetching {url}: {e}\")\n",
    "        return None\n",
    "        "
   ],
   "id": "b767f2058d060fd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# PLACEHOLDER FOR THE USER SELECTION OF INPUTS\n",
    "\n",
    "wanted_words = None # Words that should be in the URL\n",
    "is_sitemap = True\n",
    "custom_sitemap_tags = None\n"
   ],
   "id": "db835b5ee188e0f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def is_valid_link(url, wanted_words=None): # I used these parameters in case I separate the two app features, but I dont see the need right now (they are basically global variables I know but its python)\n",
    "    # Exclude common unwanted patterns\n",
    "    unwanted_patterns = ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.css', '.js', '.ico', 'tel:', 'mailto:', '#', 'twitter', 'instagram', 'facebook', 'youtube', 'pinterest', 'linkedin', 'whatsapp']\n",
    "    for pattern in unwanted_patterns:\n",
    "        if pattern in url:\n",
    "            return False\n",
    "    if wanted_words is None:\n",
    "        return True\n",
    "    # Only accept URLs that contain \"collections\" or \"products\"\n",
    "    for word in wanted_words:\n",
    "        if word in url:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "dict_href_links = {}\n",
    "\n",
    "def get_links_from_sitemap(website_link, custom_sitemap_tags=None, wanted_words=None): # modified version from the one in the other notebook\n",
    "    # Set the base of the URL depending on whether \"collections\" or \"products\" is in the link\n",
    "    website_origin = get_base_url(website_link)\n",
    "\n",
    "    html_data = get_data(website_link)\n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    list_links = []\n",
    "    \n",
    "    tags = [\"loc\"]\n",
    "    if custom_sitemap_tags is not None:\n",
    "        tags = custom_sitemap_tags\n",
    "\n",
    "    for link in soup.find_all(tags): # this contains the links inside xml files\n",
    "        link = link.text\n",
    "        # Filter out invalid links (non-product/collection pages)\n",
    "        \n",
    "        if not is_valid_link(link, wanted_words): \n",
    "            continue\n",
    "        \n",
    "        link_to_append = None\n",
    "\n",
    "        # Handle absolute URLs that start with the origin\n",
    "        if link.startswith(str(website_origin)):\n",
    "            link_to_append = link\n",
    "        \n",
    "        # Handle relative URLs that start with \"/\"\n",
    "        elif link.startswith(\"/\"):\n",
    "            # print(href)\n",
    "            link_with_www = website_origin + link[1:]\n",
    "            # print(\"adjusted link =\", link_with_www)\n",
    "            link_to_append = link_with_www\n",
    "        \n",
    "        # If link_to_append is not None, check if it's already in dict_href_links and if it's accessible\n",
    "        if link_to_append is not None:\n",
    "            if link_to_append not in dict_href_links: #  and check_website(link_to_append) - I will not check the links here, I will check them after I get all the links\n",
    "                dict_href_links[link_to_append] = None  # Mark it as seen\n",
    "\n",
    "                list_links.append(link_to_append)\n",
    "\n",
    "    # Convert list of links to a dictionary with \"Not-checked\" as the default value for each\n",
    "    dict_links = dict.fromkeys(list_links, \"Not-checked\")\n",
    "    return dict_links\n",
    "\n",
    "def get_links(website_link, wanted_words=None):\n",
    "    # Set the base of the URL depending on whether \"collections\" or \"products\" is in the link\n",
    "    website_origin = get_base_url(website_link)\n",
    "\n",
    "    html_data = get_data(website_link)\n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    list_links = []\n",
    "\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link[\"href\"]\n",
    "\n",
    "        # Filter out invalid links (non-product/collection pages)\n",
    "        if not is_valid_link(href, wanted_words):\n",
    "            continue\n",
    "        \n",
    "        link_to_append = None\n",
    "\n",
    "        # Handle absolute URLs that start with the origin\n",
    "        if href.startswith(str(website_origin)):\n",
    "            link_to_append = href\n",
    "        \n",
    "        # Handle relative URLs that start with \"/\"\n",
    "        elif href.startswith(\"/\"):\n",
    "            #print(href)\n",
    "            link_with_www = website_origin + href[1:]\n",
    "            #print(\"adjusted link =\", link_with_www)\n",
    "            link_to_append = link_with_www\n",
    "        \n",
    "        # If link_to_append is not None, check if it's already in dict_href_links and if it's accessible\n",
    "        if link_to_append is not None:\n",
    "            if link_to_append not in dict_href_links: #  and check_website(link_to_append) - I will not check the links here, I will check them after I get all the links\n",
    "                dict_href_links[link_to_append] = None  # Mark it as seen\n",
    "                list_links.append(link_to_append)\n",
    "\n",
    "    # Convert list of links to a dictionary with \"Not-checked\" as the default value for each\n",
    "    dict_links = dict.fromkeys(list_links, \"Not-checked\")\n",
    "    return dict_links\n",
    "\n",
    "def get_subpage_links(l, is_sitemap=False, custom_sitemap_tags=None, wanted_words=None, max_depth=3, current_depth=0, write_frequency=20, csv_filename=\"app_feature_test.csv\"):\n",
    "    processed_links_count = 0\n",
    "    \n",
    "    if current_depth >= max_depth:\n",
    "        return l\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        \n",
    "        if is_sitemap:\n",
    "            futures = {executor.submit(get_links_from_sitemap, link, custom_sitemap_tags, wanted_words): link for link in l if l[link] == \"Not-checked\"}\n",
    "        else: futures = {executor.submit(get_links, link, wanted_words): link for link in l if l[link] == \"Not-checked\"}\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing subpage links\"):\n",
    "            link = futures[future]\n",
    "            try:\n",
    "                dict_links_subpages = future.result()\n",
    "                # print(f\"Processed {link} with {len(dict_links_subpages)} subpages.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                continue\n",
    "            l[link] = \"Checked\"\n",
    "            l.update(dict_links_subpages)\n",
    "\n",
    "            processed_links_count += 1\n",
    "            \n",
    "            # Write to file every 'write_frequency' processed links\n",
    "            if processed_links_count >= write_frequency: # this actually writes all the links to the csv file - even the not checked ones but in my case it is sufficient\n",
    "                write_links_to_csv(l, csv_filename)\n",
    "                processed_links_count = 0  # Reset the counter\n",
    "\n",
    "    # Recursively call the function for the next depth level\n",
    "    return get_subpage_links(l, is_sitemap, custom_sitemap_tags, wanted_words, max_depth, current_depth + 1, write_frequency, csv_filename)\n",
    "\n",
    "def write_links_to_csv(links_dict, csv_filename):\n",
    "    \"\"\"Writes the current state of the links dictionary to a CSV file.\"\"\"\n",
    "    with open(csv_filename, \"w\", newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        for link in links_dict.keys():\n",
    "            csvwriter.writerow([link])\n",
    "\n",
    "    print(f\"Links saved to {csv_filename}.\")"
   ],
   "id": "a6b35fed8704cdfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = \"https://www.tyfinefurniture.com/sitemap.xml\" # For testing\n",
    "\n",
    "# create dictionary of websites\n",
    "def scrape_website_links(url, is_sitemap=False, custom_sitemap_tags=None, wanted_words=None, output_file=\"app_feature_test.csv\"):\n",
    "    # Initialize the dictionary with the starting URL\n",
    "    dict_links = {url: \"Not-checked\"}\n",
    "\n",
    "    counter, counter2 = None, 0\n",
    "\n",
    "    while counter != 0:\n",
    "        counter2 += 1\n",
    "        # Call the function to get subpage links\n",
    "        dict_links2 = get_subpage_links(dict_links, is_sitemap=is_sitemap, custom_sitemap_tags=custom_sitemap_tags, wanted_words=wanted_words)\n",
    "        \n",
    "        # Update the counter to see how many links are left unchecked\n",
    "        counter = operator.countOf(dict_links2.values(), \"Not-checked\")  # Number of \"Not-checked\" links\n",
    "\n",
    "        # Debugging statements\n",
    "        print(\"\\nTHIS IS LOOP ITERATION NUMBER\", counter2)\n",
    "        print(\"LENGTH OF DICTIONARY WITH LINKS =\", len(dict_links2))\n",
    "        print(\"NUMBER OF 'Not-checked' LINKS =\", counter, \"\\n\")\n",
    "\n",
    "        # Update the dictionary with the newly found links\n",
    "        dict_links = dict_links2\n",
    "\n",
    "    # Write the collected links to the specified CSV file\n",
    "    with open(output_file, \"w\", newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        for link in dict_links.keys():\n",
    "            csvwriter.writerow([link])\n",
    "\n",
    "    print(f\"Scraping completed. Total links scraped: {len(dict_links)}\")\n",
    "    print(f\"Links saved to {output_file}\")\n",
    "    \n",
    "    \n",
    "scrape_website_links(url, is_sitemap=True, custom_sitemap_tags=[\"loc\"], wanted_words=[\"collections\", \"products\"], output_file=\"app_feature_test.csv\")"
   ],
   "id": "398e991cd38735e2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
