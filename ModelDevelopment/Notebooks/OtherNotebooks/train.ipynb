{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training notebook",
   "id": "70924eed015d7863"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "\n",
    "label_all_tokens = False\n",
    "label_map = {'O': 0, 'B-PRODUCT': 1, 'I-PRODUCT': 2} # bert expects labels to be in the form of integers\n",
    "reverse_label_map = {v: k for k, v in label_map.items()} # we will use this to convert the model's output back to the original labels ffor metrics"
   ],
   "id": "a7474f0758d0269f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def read_conll_file(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    sentence = []\n",
    "    label = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Skip blank lines (end of a sentence)\n",
    "            if line.strip() == \"\":\n",
    "                if sentence:  # Add the sentence and its labels to the list\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "            else:\n",
    "                # Safely split the line and check if it has the expected number of columns\n",
    "                parts = line.split()\n",
    "\n",
    "                if len(parts) == 4:  # We expect 4 columns: token, column 2, column 3, label\n",
    "                    token, _, _, ner_label = parts\n",
    "                    if ner_label == \"B-I-PROD\":\n",
    "                        ner_label = \"I-PROD\"\n",
    "                    if ner_label == \"B-B-PROD\":\n",
    "                        ner_label = \"B-PROD\"\n",
    "                    if ner_label == \"I-B-PROD\":\n",
    "                        ner_label = \"I-PROD\"\n",
    "                    if ner_label == \"I-I-PROD\":\n",
    "                        ner_label = \"I-PROD\"\n",
    "\n",
    "                elif len(parts) == 3:  # If there's a missing column, handle it\n",
    "                    token, _, ner_label = parts\n",
    "                    if ner_label == \"B-I-PROD\":\n",
    "                        ner_label = \"I-PROD\"\n",
    "                    if ner_label == \"B-B-PROD\":\n",
    "                        ner_label = \"B-PROD\"\n",
    "                    if ner_label == \"I-B-PROD\":\n",
    "                        ner_label = \"I-PROD\"\n",
    "                    if ner_label == \"I-I-PROD\":\n",
    "                        ner_label = \"I-PROD\"\n",
    "                else:\n",
    "                    # Handle unexpected lines\n",
    "                    print(f\"Skipping line: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                # Add the token and its label to the sentence\n",
    "                sentence.append(token)\n",
    "                label.append(ner_label)\n",
    "\n",
    "        # If there's an unfinished sentence at the end of the file\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels[i])):\n",
    "            labels[i][j] = label_map[labels[i][j]]\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "def parse_list_from_string(list_string):\n",
    "    try:\n",
    "        # Safely evaluate the string to convert it into a Python list\n",
    "        return ast.literal_eval(list_string)\n",
    "    except (ValueError, SyntaxError):\n",
    "        print(f\"Error parsing: {list_string}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# with open('../data/100000_data_ready_for_training', 'r', encoding='utf-8', newline='') as file:\n",
    "#     reader = csv.reader(file)\n",
    "#     for row in reader:\n",
    "#         url, tokens_str, labels_str = row\n",
    "#         tokens = tokens_str.split(' ') # !!!\n",
    "#         labels = labels_str.split(' ')\n",
    "#         preprocessed_data.append((url, tokens, labels))\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    train_sentences = []\n",
    "    train_labels = []\n",
    "    # csv with link, base-link, tokens, label\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            # Skip the header\n",
    "            if row[0] == \"URL\":\n",
    "                continue\n",
    "\n",
    "            url, tokens_str, labels_str = row\n",
    "            tokens = tokens_str.split(' ') # !!!\n",
    "            labels = labels_str.split(' ')\n",
    "\n",
    "            # Add the tokens and labels to the lists\n",
    "            train_sentences.append(tokens)\n",
    "            train_labels.append(labels)\n",
    "\n",
    "    for i in range(len(train_labels)):\n",
    "        for j in range(len(train_labels[i])):\n",
    "            train_labels[i][j] = label_map[train_labels[i][j]]\n",
    "\n",
    "    return train_sentences, train_labels\n",
    "\n"
   ],
   "id": "e8ac027ca10f1d3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# train_sentences, train_labels = read_conll_file(\"data/training_data_v1.conll\")\n",
    "train_sentences, train_labels = read_csv_file(\"../../Data/InUseData/100000_data_ready_for_training.csv\")\n",
    "\n",
    "print(train_sentences[:5], train_labels[:5])\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "\n",
    "# we add the special tokens\n",
    "special_tokens = ['[URL]', '[TITLE]', '[TEXT]', '<NO_TITLE>', '<NO_URL>']\n",
    "\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "\n",
    "# this method aligns the labels after tokenization (some words may have been split into multiple tokens + the 2 special tokens)\n",
    "# def tokenize_and_align_labels(train_sentences, train_labels): # this WORKSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
    "#     tokenized_inputs = tokenizer(train_sentences, truncation=True, is_split_into_words=True)\n",
    "#\n",
    "#     labels = []\n",
    "#     for i, label in enumerate(train_labels):\n",
    "#         word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "#         previous_word_idx = None\n",
    "#         label_ids = []\n",
    "#         for word_idx in word_ids:\n",
    "#             # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "#             # ignored in the loss function.\n",
    "#             if word_idx is None:\n",
    "#                 label_ids.append(-100)\n",
    "#             # We set the label for the first token of each word.\n",
    "#             elif word_idx != previous_word_idx:\n",
    "#                 label_ids.append(label[word_idx])\n",
    "#             # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "#             # the label_all_tokens flag.\n",
    "#             else:\n",
    "#                 label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "#             previous_word_idx = word_idx\n",
    "#\n",
    "#         labels.append(label_ids)\n",
    "#\n",
    "#     tokenized_inputs[\"labels\"] = labels\n",
    "#     return tokenized_inputs\n",
    "\n",
    "def tokenize_and_align_labels(train_sentences, train_labels):\n",
    "    tokenized_inputs = tokenizer(train_sentences, truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(train_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # If this is the first token of a word, use the corresponding label\n",
    "            elif word_idx != previous_word_idx:\n",
    "                if word_idx < len(label):  # Check if the word index is within label range\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    # If the word index is out of range, append -100 (ignore token)\n",
    "                    label_ids.append(-100)\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ],
   "id": "e7e70ad442c4b739"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# split the train_sentences and train_labels before tokenization\n",
    "train_sentences_split, test_sentences_split, train_labels_split, test_labels_split = train_test_split(\n",
    "    train_sentences, train_labels, test_size=0.2\n",
    ")\n",
    "\n",
    "# tokenize and align labels for both training and test datasets\n",
    "train_data = tokenize_and_align_labels(train_sentences_split, train_labels_split)\n",
    "test_data = tokenize_and_align_labels(test_sentences_split, test_labels_split)\n",
    "\n",
    "\n",
    "print(train_data[\"input_ids\"][0], train_data[\"labels\"][0])\n",
    "\n",
    "# convert the tokenized data to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)"
   ],
   "id": "49f6ffc320d82bac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Now we load the model\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_map))\n",
    "model.resize_token_embeddings(len(tokenizer)) # This is done because of the special tokens we added\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-for-product-extraction\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [reverse_label_map[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [reverse_label_map[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))"
   ],
   "id": "443168063b7586"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "print(trainer.evaluate())"
   ],
   "id": "dfd60582d299b683"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
