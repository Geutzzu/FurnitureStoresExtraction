{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## This notebook will focus on fetching as many links as possible from the websites that did not have a sitemap (and that work)",
   "id": "3938d403579df93a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "import csv # importing the csv module\n",
    "import operator\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm # we use this to keep track of the progress of a loop\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "from urllib.parse import urlparse\n",
    "import random\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_base_url(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        return base_url\n",
    "    except Exception as e:\n",
    "        # print(f\"Error parsing URL {url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:79.0) Gecko/20100101 Firefox/79.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "]    \n",
    "\n",
    "def check_website(url):\n",
    "    headers = {\"User-Agent\": random.choice(USER_AGENTS)}  # Rotate user-agent\n",
    "    try:\n",
    "        response = requests.get(url, headers = headers ,timeout=5)  # timeout after 5 seconds\n",
    "        if 200 <= response.status_code < 300:\n",
    "            return True  # the website is accessible if the status code is between 200 and 299\n",
    "        else:\n",
    "            return False  # website returned an error (not accessible)\n",
    "    except requests.RequestException as e:\n",
    "        return False  # there was an issue with the request (e.g., domain error, timeout, etc.)\n",
    "\n",
    "def check_websites_concurrently(links):\n",
    "    accessible_links = []\n",
    "    with ThreadPoolExecutor(max_workers=30) as executor:\n",
    "        futures = {executor.submit(check_website, link): link for link in links}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(links), desc=\"Checking links\"):\n",
    "            # futures work like promises in JavaScript\n",
    "            link = futures[future] # get the link associated with the future from the futures dictionary (futures is a dictionary with the future as the key and the link as the value)\n",
    "            if future.result():\n",
    "                accessible_links.append(link)\n",
    "    return accessible_links\n"
   ],
   "id": "47b2da180d57f6b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# getting only the sitemaps from the csv file\n",
    "no_sitemaps = []\n",
    "with open('../sitemap_results.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if row[1] == 'No sitemap found':\n",
    "            no_sitemaps.append(row[0])\n",
    "            \n",
    "no_sitemaps = no_sitemaps[2:]\n",
    "basic_csv = []\n",
    "\n",
    "with open ('../data/furniture stores pages.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        basic_csv.append(row)\n",
    "        \n",
    "basic_csv = basic_csv[1:]\n",
    "\n",
    "print(len(no_sitemaps)) # 320\n",
    "\n",
    "for i in range(len(no_sitemaps)):\n",
    "    for j in range(len(basic_csv)):\n",
    "        if no_sitemaps[i] in basic_csv[j][0]:\n",
    "            print(no_sitemaps[i], basic_csv[j][0])\n",
    "            no_sitemaps[i] = basic_csv[j][0]\n",
    "            break\n",
    "            \n",
    "print(len(no_sitemaps)) # 320\n",
    "        \n",
    "# trimming all websites until the words collections or products show up in the URL\n",
    "no_sitemaps = [trim_url(url) for url in no_sitemaps]\n",
    "\n",
    "print(no_sitemaps)\n",
    "print(len(no_sitemaps)) # 320\n",
    "\n",
    "\n"
   ],
   "id": "7de1f7f640b0a9b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_links = check_websites_concurrently(no_sitemaps)\n",
    "\n",
    "print(len(final_links)) # 73"
   ],
   "id": "b6e083b73a79dadd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# saving the final links to a csv file\n",
    "\n",
    "with open('../data/no_sitemap_links.csv', mode='w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    csv_writer.writerow(['URL'])\n",
    "    for link in final_links:\n",
    "        csv_writer.writerow([link])"
   ],
   "id": "b7a0c93e1e726a2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# getting each pages links and going through them\n",
    "# major props to this medium post: https://python.plainenglish.io/scraping-the-subpages-on-a-website-ea2d4e3db113\n",
    "\n",
    "# function to get page content\n",
    "def get_data(url):\n",
    "    headers = {\"User-Agent\": random.choice(USER_AGENTS)}  # Rotate user-agent\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=3)\n",
    "\n",
    "        # Handle rate-limiting (HTTP 429) by pausing and retrying\n",
    "        if response.status_code == 429:\n",
    "            tqdm.write(f\"FROM GET_DATA: Rate limit reached. Sleeping before retrying {url}\")\n",
    "            # print(f\"FROM GET_DATA: Rate limit reached. Sleeping before retrying {url}\")\n",
    "            time.sleep(random.uniform(4, 8))  # Random delay to avoid detection\n",
    "            return get_data(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.content  # Return HTML content if successful\n",
    "\n",
    "        tqdm.write(f\"FROM GET_DATA: Failed to retrieve {url}, Status Code: {response.status_code}\")\n",
    "        # print(f\"FROM GET_DATA: Failed to retrieve {url}, Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        tqdm.write(f\"FROM GET_DATA: Error fetching {url}: {e}\")\n",
    "        # print(f\"FROM GET_DATA: Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def is_valid_link(url):\n",
    "    # exclude common unwanted patterns\n",
    "    unwanted_patterns = ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.css', '.js', '.ico', 'tel:', 'mailto:', '#']\n",
    "    for pattern in unwanted_patterns:\n",
    "        if pattern in url:\n",
    "            return False\n",
    "    # only accept URLs that contain \"collections\" or \"products\"\n",
    "    if '/collections/' in url or '/products/' in url:\n",
    "        return True\n",
    "    return True\n",
    "\n",
    "\n",
    "dict_href_links = {}\n",
    "\n",
    "def get_links(website_link):\n",
    "    # set the base of the URL depending on whether \"collections\" or \"products\" is in the link\n",
    "    website_origin = website_link\n",
    "    collections_index = website_link.find('/collections/')\n",
    "    if collections_index != -1:\n",
    "        website_origin = website_link[:collections_index + 1] \n",
    "\n",
    "    html_data = get_data(website_link)\n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    list_links = []\n",
    "\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link[\"href\"]\n",
    "\n",
    "        # filter out invalid links (non-product/collection pages)\n",
    "        if not is_valid_link(href):\n",
    "            continue\n",
    "        \n",
    "        link_to_append = None\n",
    "\n",
    "        # handle absolute URLs that start with the origin\n",
    "        if href.startswith(str(website_origin)):\n",
    "            link_to_append = href\n",
    "        \n",
    "        # handle relative URLs that start with \"/\"\n",
    "        elif href.startswith(\"/\"):\n",
    "            #print(href)\n",
    "            link_with_www = website_origin + href[1:]\n",
    "            #print(\"adjusted link =\", link_with_www)\n",
    "            link_to_append = link_with_www\n",
    "        \n",
    "        # if link_to_append is not None, check if it's already in dict_href_links and if it's accessible\n",
    "        if link_to_append is not None:\n",
    "            if link_to_append not in dict_href_links: #  and check_website(link_to_append) - I will not check the links here, I will check them after I get all the links\n",
    "                dict_href_links[link_to_append] = None  # mark it as seen\n",
    "                list_links.append(link_to_append)\n",
    "\n",
    "    # convert list of links to a dictionary with \"Not-checked\" as the default value for each\n",
    "    dict_links = dict.fromkeys(list_links, \"Not-checked\")\n",
    "    return dict_links\n",
    "\n",
    "def get_subpage_links(l, max_depth=3, current_depth=0, write_frequency=500, csv_filename=\"no_sitemap_data.csv\"):\n",
    "    processed_links_count = 0\n",
    "    \n",
    "    if current_depth >= max_depth:\n",
    "        return l\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        futures = {executor.submit(get_links, link): link for link in l if l[link] == \"Not-checked\"}\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing subpage links\"):\n",
    "            link = futures[future]\n",
    "            try:\n",
    "                dict_links_subpages = future.result()\n",
    "                # print(f\"Processed {link} with {len(dict_links_subpages)} subpages.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                continue\n",
    "            l[link] = \"Checked\"\n",
    "            l.update(dict_links_subpages)\n",
    "\n",
    "            processed_links_count += 1\n",
    "            \n",
    "            # write to file every 'write_frequency' processed links\n",
    "            if processed_links_count >= write_frequency: # this actually writes all the links to the csv file - even the not checked ones but in my case it is sufficient\n",
    "                write_links_to_csv(l, csv_filename)\n",
    "                processed_links_count = 0  # reset the counter\n",
    "\n",
    "    # recursively call the function for the next depth level\n",
    "    return get_subpage_links(l, max_depth, current_depth + 1, write_frequency, csv_filename)\n",
    "\n",
    "def write_links_to_csv(links_dict, csv_filename):\n",
    "    \"\"\"writes the current state of the links dictionary to a CSV file.\"\"\"\n",
    "    with open(csv_filename, \"w\", newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        for link in links_dict.keys():\n",
    "            csvwriter.writerow([link])\n",
    "\n",
    "    print(f\"Links saved to {csv_filename}.\")"
   ],
   "id": "340538d9ffcca601",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we gather all the links from some pages\n",
    "\n",
    "# we test with the first page inside the final_links csv\n",
    "\n",
    "websites = []\n",
    "with open('../data/no_sitemap_links.csv', mode='r', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        websites.append(row[0])\n",
    "    \n",
    "# create dictionary of websites\n",
    "dict_links = dict.fromkeys(websites, \"Not-checked\")\n",
    "\n",
    "counter, counter2 = None, 0\n",
    "\n",
    "while counter != 0:\n",
    "    counter2 += 1\n",
    "    dict_links2 = get_subpage_links(dict_links)\n",
    "    counter = operator.countOf(dict_links2.values(), \"Not-checked\")  # number of \"Not-checked\" links\n",
    "    \n",
    "    # print some statements for debugging\n",
    "    print(\"\")\n",
    "    print(\"THIS IS LOOP ITERATION NUMBER\", counter2)\n",
    "    print(\"LENGTH OF DICTIONARY WITH LINKS =\", len(dict_links2))\n",
    "    print(\"NUMBER OF 'Not-checked' LINKS = \", counter)\n",
    "    print(\"\")\n",
    "    \n",
    "    dict_links = dict_links2\n",
    "\n",
    "# write only the links to a CSV file after the loop completes\n",
    "with open(\"\", \"w\", newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    # write each link as a new row in the CSV file\n",
    "    for link in dict_links.keys():\n",
    "        csvwriter.writerow([link])\n",
    "\n",
    "print(\"Links saved to link_data.csv.\")"
   ],
   "id": "338e7da6916d3009",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subpage links:   6%|▌         | 7219/130497 [09:44<3:51:19,  8.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM GET_DATA: Error fetching https://themodern.net.au/collections/weave-by-warwick/products/weave-ava-cushion-50-x-50cm: HTTPSConnectionPool(host='themodern.net.au', port=443): Read timed out. (read timeout=3)\n",
      "Error fetching https://themodern.net.au/collections/weave-by-warwick/products/weave-ava-cushion-50-x-50cm: object of type 'NoneType' has no len()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subpage links:   6%|▌         | 7285/130497 [09:49<2:46:11, 12.36it/s]\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
