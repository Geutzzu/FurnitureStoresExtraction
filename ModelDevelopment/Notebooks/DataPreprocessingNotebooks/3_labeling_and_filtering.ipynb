{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tokenization, labeling and filtering\n",
    "- My approach revolves around labeling the h1 tag from websites that get through my filtering and additionally (in some of my training datasets) label similar string to the h1 tag that is allready labeled.\n",
    "- Also relevant is the format of my training data. Each entry will contain the following:\n",
    "    1. URL last path (i.e. modern-wooden-chair)\n",
    "    2. Page Title (i.e. Modern Wooden Chair, usually similar or identical to the h1 tag)\n",
    "    3. H1 tag surrounded by page text (i.e. Modern Wooden Chair, Price 1000$, Description: Beautiful modern wooden chair ... )\n",
    "- Subject to change will be the number of tokens left and right of the h1 tag that will be labeled and how to label substrings that are similar to the h1 tag (fuzzy matching or even more advanced methods).\n",
    "- The labeling will be done in the BIO / CoNLL format (B-PRODUCT, I-PRODUCT, O) where B-PRODUCT is the beginning of the product name, I-PRODUCT is the continuation of the product name and O is outside of the product name."
   ],
   "id": "ea28a462fd052f94"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# importing libraries\n",
    "import csv\n",
    "csv.field_size_limit(5000000)\n",
    "import ast\n",
    "import operator\n",
    "import re\n",
    "import threading\n",
    "import random\n",
    "from rapidfuzz import fuzz\n",
    "import sys\n",
    "import requests\n",
    "from tqdm import tqdm \n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "import spacy # we use this for word similarity\n",
    "from collections import defaultdict\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### A little bit of data preprocessing (i.e. manually removing data that is likely wrong by removing entries with less that 1 words in the title, with strange symbols, etc.)\n",
    "- By looking at the data manually I noticed what the wrong titles contain. The dataset will still be a bit noisy but I consider we have enough data to work with.\n",
    "At the bottom of the code cell you can also see the number of distinct websites in the dataset just so you can get an idea of the diversity of the dataset. When I wrote the code it was 288 / 705 (the initial dataset). This should be enough diversity for the model to learn from.\n"
   ],
   "id": "d8d6649d1af51537"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = []\n",
    "distinct_websites = set()\n",
    "\n",
    "BAD_TEXT_PATTERNS_IN_TITLE = ['releases', 'products', 'product', 'collections', 'collection', 'item', 'personalization', 'personalize', 'personalized', 'customize', 'customized', 'customise', 'customised', 'shop', 'store', 'stores', 'home', 'page', 'pages', 'about', 'contact', 'contact us', 'contact me', 'contact info', 'furniture', 'sofas', 'chairs', 'armchairs', 'ottomans', 'furniture' 'gift', 'card', ] #  all generic names that would indicate that the h1 tag does not contain a product - we can afford to lose a few products in the dataset\n",
    "\n",
    "def clean_text(s):\n",
    "    # this pattern keeps only normal alphanumerical characters and some special symbols\n",
    "    allowed_pattern = r\"[^a-zA-Z0-9\\s,.:;\\'\\\"!?()\\-&+]\"\n",
    "    return re.sub(allowed_pattern, '', s)\n",
    "\n",
    "def get_base_url(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        return base_url\n",
    "    except Exception as e:\n",
    "        # print(f\"Error parsing URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def literal_eval(item): # for parsing the tuple from the csv file (which is now a string)\n",
    "    try:\n",
    "        # check if the string looks like a tuple or list\n",
    "        if item.startswith('(') and item.endswith(')'):\n",
    "            return ast.literal_eval(item)\n",
    "        return None\n",
    "    except (ValueError, SyntaxError):\n",
    "        # Return the original string if evaluation fails\n",
    "        return None\n",
    "    \n",
    "def find_all_h1_positions(text, h1_tag): # original_position is a tuple (start, end)\n",
    "    positions = []\n",
    "    start_pos = 0\n",
    "    # search for all occurrences of h1_tag in text - if they match exactly we label them regardless\n",
    "    while True:\n",
    "        start_idx = text.find(h1_tag, start_pos)\n",
    "        if start_idx == -1:\n",
    "            break\n",
    "        end_idx = start_idx + len(h1_tag) \n",
    "        positions.append((start_idx, end_idx))\n",
    "        start_pos = end_idx + 1\n",
    "    return h1_tag, positions\n",
    "\n",
    "\n",
    "with open('../data/preprocessed_data_from_all_sitemaps_100000.csv', 'r', encoding='utf-8', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        row[1] = literal_eval(row[1])\n",
    "        if row[1] is None:\n",
    "            continue\n",
    "        row[1] = (clean_text(row[1][0]), row[1][1], row[1][2]) # url, h1_tag_position, title, url_last_path, page_text = row\n",
    "        row[2] = clean_text(row[2])\n",
    "        row[3] = clean_text(row[3])\n",
    "        row[4] = clean_text(row[4])\n",
    "        \n",
    "        h1_tag, positions = find_all_h1_positions(row[4], row[1][0])\n",
    "        row[1] = (h1_tag, positions) # string with list\n",
    "        if row[1] is None or row[1][1] is None or 'G Plan Chloe' in row[1]: # this website haunts me in my data with G Plan Chloe\n",
    "            continue\n",
    "        \n",
    "        # finally we remove anything that has a len < 2\n",
    "        if len(row[1][0].split()) < 2:\n",
    "            continue\n",
    "        \n",
    "        ok = True\n",
    "        for word in row[1][0].split():\n",
    "            if word.lower() in BAD_TEXT_PATTERNS_IN_TITLE:\n",
    "                ok = False\n",
    "                break\n",
    "        if not ok:\n",
    "            continue    \n",
    "        \n",
    "        distinct_websites.add(get_base_url(row[0]))\n",
    "        data.append(row)\n",
    "        \n",
    "print(len(distinct_websites))"
   ],
   "id": "e7377e338b9504cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Methods for labeling the data\n",
    "- Now there is a choice to be made. We can only strictly label the h1 tag or we can label similar strings to the h1 tag. I will try both approaches and see which one works better.\n",
    "- The labeling for similar strings will only be done on the title and the url last path since they are shorter, and more likely to contain the product name, thus making it worthwhile to label them (plus the algorithm to label similar substring is inefficient and slow at the time of writing).\n",
    "- The ideal approach of labeling similar tokens and with what method (or how lenient or strict you are with the labeling) is something that can only be determined by testing and experimenting."
   ],
   "id": "e10dfbdfc784ee84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fuzzy_match(text_tokens, h1_tokens, similarity_threshold=80):\n",
    "    longest_match = None\n",
    "    max_similarity = similarity_threshold  # similarity threshold for the longest match\n",
    "    \n",
    "    for i in range(len(text_tokens)):\n",
    "        for j in range(i+1, len(text_tokens)+1):\n",
    "            text_sub_seq = ' '.join(text_tokens[i:j])\n",
    "            # compare with h1 tokens using fuzzy matching\n",
    "            similarity = fuzz.partial_ratio(text_sub_seq.lower(), ' '.join(h1_tokens).lower())\n",
    "            \n",
    "            # if the similarity is above the threshold, we check if it's the longest match so we don't label too short sequences ( it did cause problems during testing with smaller substrings)\n",
    "            if similarity >= similarity_threshold:\n",
    "                if longest_match is None or len(text_sub_seq) > len(longest_match[0]):\n",
    "                    longest_match = (text_sub_seq, similarity)\n",
    "                elif len(text_sub_seq) == len(longest_match[0]) and similarity > max_similarity:\n",
    "                    longest_match = (text_sub_seq, similarity)\n",
    "                    max_similarity = similarity\n",
    "\n",
    "    return longest_match if longest_match else None\n",
    "\n",
    "\n",
    "def label_text(text, to_label): \n",
    "    # split the main text and the text to be labeled into tokens\n",
    "    tokens = text.split()\n",
    "    label_tokens = to_label.split()\n",
    "\n",
    "    # initialize the labels list with 'O' for 'Outside'\n",
    "    labels = ['O'] * len(tokens)\n",
    "    # iterate through the tokens and label all occurrences of the label_tokens\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # convert both the token slice and label tokens to lowercase for comparison\n",
    "        if [t.lower() for t in tokens[i:i+len(label_tokens)]] == [lt.lower() for lt in label_tokens]:\n",
    "            labels[i] = 'B-PRODUCT'  # mark the beginning of the label\n",
    "            for j in range(1, len(label_tokens)):\n",
    "                labels[i+j] = 'I-PRODUCT'  # mark the rest of the label tokens\n",
    "            i += len(label_tokens)  # skip the tokens that were labeled\n",
    "        else:\n",
    "            i += 1  # move to the next token\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "# for simplicity, we will only perform fuzzy matching on the title and url last path strings \n",
    "def label_url_or_title(text, to_label):\n",
    "    # split the text and label into tokens\n",
    "    tokens = text.split()\n",
    "    label_tokens = to_label.split()\n",
    "\n",
    "    # initialize labels with 'O' for 'Outside'\n",
    "    labels = ['O'] * len(tokens)\n",
    "\n",
    "    # perform fuzzy matching between the text and the label\n",
    "    match_result = fuzzy_match(tokens, label_tokens)\n",
    "\n",
    "    if match_result is None or len(match_result[0].split()) < 3: # we don't want small matches since they are wrong most of the time\n",
    "        return tokens, labels\n",
    "    # get the best matching subsequence and its similarity score\n",
    "    matched_subseq, similarity = match_result\n",
    "    # tokenize the matched subsequence\n",
    "    matched_tokens = matched_subseq.split()\n",
    "    \n",
    "    # Find the starting index of the matched subsequence in the main text\n",
    "    start_index = None\n",
    "    for i in range(len(tokens) - len(matched_tokens) + 1):\n",
    "        if tokens[i:i+len(matched_tokens)] == matched_tokens:\n",
    "            start_index = i\n",
    "            break\n",
    "    if start_index is not None:\n",
    "        # Label the matched subsequence in the main text\n",
    "        labels[start_index] = 'B-PRODUCT'\n",
    "        for j in range(1, len(matched_tokens)):\n",
    "            labels[start_index + j] = 'I-PRODUCT'\n",
    "\n",
    "    return tokens, labels\n",
    "\n"
   ],
   "id": "feafc6cf62b8fa35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training entry format\n",
    "- The training data will be in the format of a csv file with the following columns:\n",
    "    1. URL\n",
    "    2. Tokens (space-separated)\n",
    "    3. Labels (space-separated)\n",
    "- We will have 5 special tokens that will be used to separate the different parts of the text that we want to label:\n",
    "    1. [URL] - for the URL last path\n",
    "    2. [TITLE] - for the page title\n",
    "    3. [TEXT] - for the page text\n",
    "    4. <NO_URL> - if the URL last path is missing\n",
    "    5. <NO_TITLE> - if the page title is missing\n",
    "- Token entries will look like this:\n",
    "    - [URL] modern-wooden-chair [URL] [TITLE] Modern Wooden Chair [TITLE] [TEXT] Modern Wooden Chair, Price 1000$, Description: Beautiful modern wooden chair ... [TEXT]\n",
    "- Label entries will look like this:\n",
    "    - O B-PRODUCT I-PRODUCT I-PRODUCT O O B-PRODUCT I-PRODUCT I-PRODUCT O O B-PRODUCT I-PRODUCT I-PRODUCT O O O O O O O O O O O O .... O\n",
    "- This is the format I came up with that made the most logical sense to me. It captures all the information a page has to offer about a product. The only possible issue would be making the model over rely on the URL last path and the page title, but there have been taken measures to prevent that (i.e. randomly setting the page title to <NO_TITLE> or the URL last path to <NO_URL>)."
   ],
   "id": "f2f59fa4b8a24b01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# left and right tokens represent the number of tokens before and after the h1 tag position that will be fed to the model\n",
    "def tokenize_and_label(text, h1_tag_position, title, url_last_path, tokens_left=15, tokens_right=25):\n",
    "    to_label = h1_tag_position[0]\n",
    "    \n",
    "    tokens_url_last_path, labels_url_last_path = [], []\n",
    "    tokens_title, labels_title = [], []\n",
    "    \n",
    "    if url_last_path is None or url_last_path == '': # if the url last path is missing\n",
    "        tokens_url_last_path = ['<NO_URL>']\n",
    "        labels_url_last_path = ['O']\n",
    "    elif isinstance(url_last_path, str): # we label using fuzzy matching if nothing was labeled\n",
    "        tokens_url_last_path, labels_url_last_path = label_text(url_last_path, to_label) # this labels normally\n",
    "        is_any_non_o_label = any(label != 'O' for label in labels_url_last_path) # check if there are any non 'O' labels\n",
    "        if not is_any_non_o_label:\n",
    "            tokens_url_last_path, labels_url_last_path = label_url_or_title(url_last_path, to_label)\n",
    "    else: \n",
    "        tokens_url_last_path = ['<NO_URL>']\n",
    "        labels_url_last_path = ['O']\n",
    "    \n",
    "    if title is None or title == '':\n",
    "        tokens_title = ['<NO_TITLE>']\n",
    "        labels_title = ['O']\n",
    "    elif isinstance(title, str): # we label using fuzzy matching if nothing was labeled - same as above\n",
    "        tokens_title, labels_title = label_text(title, to_label) # this labels normally\n",
    "        is_any_non_o_label = any(label != 'O' for label in labels_title) # check if there are any non 'O' labels\n",
    "        if not is_any_non_o_label:\n",
    "            tokens_title, labels_title = label_url_or_title(title, to_label) # this labels using fuzzy matching\n",
    "    else: \n",
    "        tokens_title = ['<NO_TITLE>']\n",
    "        labels_title = ['O']\n",
    "    \n",
    "    tokens_text, labels_text = label_text(text, to_label)\n",
    "    \n",
    "    try:\n",
    "        first_label_index = labels_text.index('B-PRODUCT')\n",
    "    except ValueError:\n",
    "        first_label_index = 0  # if no labeled entity, start from the beginning\n",
    "    try:\n",
    "        last_label_index = max(idx for idx, label in enumerate(labels_text) if label in ['B-PRODUCT', 'I-PRODUCT'])\n",
    "    except ValueError:\n",
    "        last_label_index = len(tokens_text) - 1  # If no labeled entity, end at the last token\n",
    "\n",
    "    # Calculate the window to slice\n",
    "    start_index = max(0, first_label_index - tokens_left)\n",
    "    end_index = min(len(tokens_text), last_label_index + tokens_right + 1) # or len(tokens_text) \n",
    "\n",
    "\n",
    "    tokens_text = tokens_text[start_index:end_index] # slice the tokens\n",
    "    labels_text = labels_text[start_index:end_index] # slice the labels\n",
    "    \n",
    "    end_of_window = 100 # maximum length of the window\n",
    "    while end_of_window < len(labels_text) and labels_text[end_of_window] != 'O': # we want to end the window at the end of the product name\n",
    "        end_of_window +=1\n",
    "    \n",
    "    if end_of_window < len(labels_text):\n",
    "        tokens_text = tokens_text[:end_of_window]\n",
    "        labels_text = labels_text[:end_of_window]\n",
    "    \n",
    "    if random.randint(1, 100) == 1:\n",
    "        tokens_title = ['<NO_TITLE>']\n",
    "        labels_title = ['O']\n",
    "    \n",
    "    tokens = ['[URL]'] + tokens_url_last_path + ['[URL]', '[TITLE]'] + tokens_title + ['[TITLE]', '[TEXT]'] + tokens_text + ['[TEXT]'] # concatenate all tokens and matching the format\n",
    "    \n",
    "    labels = ['O'] + labels_url_last_path + ['O', 'O'] + labels_title + ['O', 'O'] + labels_text + ['O'] # concatenate all labels and matching the format\n",
    "    \n",
    "\n",
    "    return tokens, labels # return the tokens and labels\n"
   ],
   "id": "9ae038adbc570abf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Creating our dataset for training\n",
    "- One thing to note is that we have to join the tokens and labels into a single string so that we can write them to a csv file. This is due to how the BERT tokenizer works (see the training notebook). "
   ],
   "id": "31717dc524b48b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('../data/100000_data_ready_for_training.csv', 'w', encoding='utf-8', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for row in data:\n",
    "        url, h1_tag_positions, title, url_last_path, page_text = row\n",
    "        \n",
    "        tokens, labels = tokenize_and_label(page_text, h1_tag_positions, title, url_last_path)\n",
    "        # tokens, labels = clean_tokens_with_labels(tokens, labels)\n",
    "        tokens_str = ' '.join(tokens)  # join tokens into a single string\n",
    "        labels_str = ' '.join(labels)  # join labels into a single string\n",
    "        writer.writerow([url, tokens_str, labels_str])\n"
   ],
   "id": "7e402b5bfb985703",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Testing code",
   "id": "6f665bb678664012"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocessed_data = []\n",
    "\n",
    "with open('../data/100000_data_ready_for_training.csv', 'r', encoding='utf-8', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        url, tokens_str, labels_str = row\n",
    "        tokens = tokens_str.split(' ') # !!!\n",
    "        labels = labels_str.split(' ')\n",
    "        preprocessed_data.append((url, tokens, labels))"
   ],
   "id": "70e9a98d4e85b2c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
