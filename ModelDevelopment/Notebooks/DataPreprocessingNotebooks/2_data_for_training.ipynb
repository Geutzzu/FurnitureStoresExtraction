{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Web scraping for most of the sitemap websites\n",
    "- This notebook picks up from the dispersed_link_data.csv file from the last notebook and aims to gather the important raw data from each link (or as many as possible) in a new CSV file that will be sent to the next notebook for tokenization and labeling."
   ],
   "id": "e58d6fa6cae01bda"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# importing libraries\n",
    "import csv \n",
    "import operator\n",
    "import re\n",
    "import threading\n",
    "import random\n",
    "import sys\n",
    "import requests\n",
    "from tqdm import tqdm \n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "from collections import defaultdict\n",
    "import time\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading the links for scraping"
   ],
   "id": "70014b6139f4992b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "links = []\n",
    "\n",
    "with open('../dispersed_link_data.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        links.append(row[0])"
   ],
   "id": "b45c1a630cbb50ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Additional method for storing data",
   "metadata": {
    "collapsed": false
   },
   "id": "ce5baaf9812c5c3a"
  },
  {
   "cell_type": "code",
   "source": [
    "def write_data_to_csv(data_to_write, csv_filename):\n",
    "    with open(csv_filename, \"a\", newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        for row in data_to_write:\n",
    "            csvwriter.writerow(row)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e2c507de6cfaec1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Methods that will be used for scraping\n",
    "- The same get_data method is used alongside the get_base_url.\n",
    "- The code goes through the links and extracts the relevant sections (h1, title, url_last_path, page_text) from the HTML content.\n",
    "- It also eliminates the header content, scripts, styles etc."
   ],
   "id": "7e730773ae77fcc8"
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "def get_base_url(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        return base_url\n",
    "    except Exception as e:\n",
    "        # print(f\"Error parsing URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace multiple spaces and line breaks with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:79.0) Gecko/20100101 Firefox/79.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "]\n",
    "\n",
    "def get_data(url):\n",
    "    headers = {\"User-Agent\": random.choice(USER_AGENTS)}  # it uses a random user agent from the list above - it avoids getting IP banned from most websites inside the CSV if I scrape multiple times\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=3)\n",
    "\n",
    "        # handle rate-limiting (HTTP 429) by pausing and retrying\n",
    "        if response.status_code == 429:\n",
    "            tqdm.write(f\"FROM GET_DATA: Rate limit reached. Sleeping before retrying {url}\")\n",
    "            # print(f\"FROM GET_DATA: Rate limit reached. Sleeping before retrying {url}\")\n",
    "            time.sleep(random.uniform(4, 8))  # random delay to avoid detection\n",
    "            return get_data(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.content  # return content if successful\n",
    "\n",
    "        tqdm.write(f\"FROM GET_DATA: Failed to retrieve {url}, Status Code: {response.status_code}\")\n",
    "        # print(f\"FROM GET_DATA: Failed to retrieve {url}, Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        tqdm.write(f\"FROM GET_DATA: Error fetching {url}: {e}\")\n",
    "        # print(f\"FROM GET_DATA: Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def has_letters(input_string):\n",
    "    return any(char.isalpha() for char in input_string)\n",
    "\n",
    "def extract_data(url):\n",
    "    html_data = get_data(url)\n",
    "    if html_data is None:\n",
    "        return None\n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    \n",
    "    # finding the title in the meta tags - 99% of websites have one and it has relevant data for the model most of the time\n",
    "    title = soup.find('title')\n",
    "    if title:\n",
    "        title = title.get_text()\n",
    "    else: title = None\n",
    "        \n",
    "    # remove scripts, styles, and irrelevant content\n",
    "    for script in soup([\"script\", \"style\", \"footer\", \"nav\", \"header\", \"noscript\", \"head\"]):\n",
    "        script.extract()\n",
    "\n",
    "    h1_tags = [clean_text(h1.get_text()) for h1 in soup.find_all('h1')]\n",
    "    page_text = clean_text(soup.get_text(separator=' '))\n",
    "    \n",
    "    h1_tag = None\n",
    "    # if no h1 tags are found, return None - since this is our label for the model\n",
    "    if h1_tags:\n",
    "        h1_tag = h1_tags[0] \n",
    " \n",
    "    # we get the last path of the url\n",
    "    url_index = url.rfind('/')\n",
    "    url_last_path = url[url_index + 1:].replace('-', ' ').replace('_', ' ')\n",
    "    if not has_letters(url_last_path): # there are some paths that contain only numbers which dont help the model\n",
    "        url_last_path = None\n",
    "    \n",
    "    h1_tag_position = None\n",
    "    if h1_tag and page_text: \n",
    "        if h1_tag in page_text:\n",
    "            start_idx = page_text.index(h1_tag)\n",
    "            end_idx = start_idx + len(h1_tag)\n",
    "            h1_tag_position = (h1_tag, start_idx, end_idx)\n",
    "    \n",
    "    return h1_tag_position, title, url_last_path, page_text"
   ],
   "id": "da2a0de946c3761f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "final_data = []\n",
    "links = links[:100000]  # limit the number of links to process (more than 100000 took me way too long to compute) \n",
    "\n",
    "final_data_lock = threading.Lock()  # lock to ensure thread-safe appending to the list\n",
    "\n",
    "# function to process a single URL\n",
    "def process_url(link):\n",
    "    try:\n",
    "        result = extract_data(link)\n",
    "        if result is not None:\n",
    "            h1_tag, title, url_last_path, page_text = result\n",
    "            with final_data_lock:\n",
    "                final_data.append([link, h1_tag, title, url_last_path, page_text])\n",
    "        else:\n",
    "            tqdm.write(f\"FROM:PROCESS_URL: Skipped processing {link} because extract_data returned None\")\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"FROM:PROCESS_URL: Error processing {link}: {e}\")\n",
    "\n",
    "\n",
    "# read from CSV and process in parallel\n",
    "with open('../dispersed_link_data.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    with ThreadPoolExecutor(max_workers=32) as executor: \n",
    "        futures = []\n",
    "        for link in links:\n",
    "            futures.append(executor.submit(process_url, link))  \n",
    "        for _ in tqdm(concurrent.futures.as_completed(futures), total=len(links), desc=\"Processing URLs\"):\n",
    "            pass  # we don't need the result here since we're appending directly to final_data"
   ],
   "id": "3f6758d26e6670b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Saving the data to a CSV file for the next notebook\n",
    "- Important to note here is that this file gets big very quickly, at 100000 entries excel was already struggling to open it (and stole all my ram), and it took about 2 hours to compute."
   ],
   "id": "d4b73955ce0e8898"
  },
  {
   "cell_type": "code",
   "source": [
    "with open('../data/preprocessed_data_from_all_sitemaps.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    csv_writer.writerow(['URL', 'h1', 'title', 'url_last_path', 'page_text'])\n",
    "    for row in final_data:\n",
    "        csv_writer.writerow(row)\n",
    "        \n",
    "print(\"Processing completed!\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6c10df05c3a6f7a2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
